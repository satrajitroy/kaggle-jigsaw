{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-17T14:29:13.734040Z",
     "start_time": "2025-08-17T14:29:12.722336Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "trn = \"C:/Users/satra/Downloads/jigsaw-agile-community-rules/train.csv\"\n",
    "tst = \"C:/Users/satra/Downloads/jigsaw-agile-community-rules/test.csv\"\n",
    "df_trn = pd.read_csv(trn)\n",
    "df_tst = pd.read_csv(tst)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:29:14.080233Z",
     "start_time": "2025-08-17T14:29:13.919983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trn_rows = []\n",
    "for idx, row in df_trn.iterrows():\n",
    "  trn_rows.append({\n",
    "    'body': row['positive_example_1'],\n",
    "    'rule': row['rule'],\n",
    "    'subreddit': row['subreddit'],\n",
    "    'label': 1\n",
    "  })\n",
    "\n",
    "  trn_rows.append({\n",
    "    'body': row['positive_example_2'],\n",
    "    'rule': row['rule'],\n",
    "    'subreddit': row['subreddit'],\n",
    "    'label': 1\n",
    "  })\n",
    "\n",
    "  trn_rows.append({\n",
    "    'body': row['negative_example_1'],\n",
    "    'rule': row['rule'],\n",
    "    'subreddit': row['subreddit'],\n",
    "    'label': 0\n",
    "  })\n",
    "\n",
    "  trn_rows.append({\n",
    "    'body': row['negative_example_2'],\n",
    "    'rule': row['rule'],\n",
    "    'subreddit': row['subreddit'],\n",
    "    'label': 0\n",
    "  })\n",
    "\n",
    "trn_df = pd.DataFrame(trn_rows)\n",
    "\n",
    "val_rows = []\n",
    "for idx, row in df_trn.iterrows():\n",
    "  val_rows.append({\n",
    "    'body': row['body'],\n",
    "    'rule': row['rule'],\n",
    "    'subreddit': row['subreddit'],\n",
    "    'label': row['rule_violation']\n",
    "  })\n",
    "\n",
    "val_df = pd.DataFrame(val_rows)\n",
    "\n",
    "tst_rows = []\n",
    "for idx, row in df_tst.iterrows():\n",
    "  tst_rows.append({\n",
    "    'body': row['body'],\n",
    "    'rule': row['rule'],\n",
    "    'subreddit': row['subreddit']\n",
    "  })\n",
    "\n",
    "tst_df = pd.DataFrame(tst_rows)\n",
    "print (f'Train shape: {trn_df.shape}, Val shape: {val_df.shape}, Test shape: {tst_df.shape}')"
   ],
   "id": "1bddb764c85731c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (8116, 4), Val shape: (2029, 4), Test shape: (10, 3)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:29:14.114320Z",
     "start_time": "2025-08-17T14:29:14.103312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# ---------- text normalization (urls -> LINK, unify synonyms) ----------\n",
    "URL_RE     = re.compile(r'(?i)\\b(?:https?://|www\\.)[^\\s)]+')\n",
    "MD_LINK_RE = re.compile(r'\\[([^\\]]+)\\]\\((?:https?://|www\\.)[^\\s)]+\\)')\n",
    "\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        s = \"\" if s is None else str(s)\n",
    "    s = MD_LINK_RE.sub(r'\\1 LINK', s)\n",
    "    s = URL_RE.sub(' LINK ', s)\n",
    "    s = re.sub(r'(?i)\\burls?\\b', ' LINK ', s)\n",
    "    s = re.sub(r'(?i)\\blinks?\\b', ' LINK ', s)\n",
    "    s = re.sub(r'(?i)\\breferences?\\b', ' REFERENCE ', s)\n",
    "    s = re.sub(r'(?i)\\bcitations?\\b', ' REFERENCE ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def clean_text_col(s: pd.Series) -> pd.Series:\n",
    "    return s.astype(str).fillna(\"\").map(normalize_text)\n",
    "\n",
    "# ---------- extra high-signal flags (link in body vs rule forbids) ----------\n",
    "FORBID_RE = re.compile(\n",
    "    r'(?i)\\b(?:no|without|avoid|prohibit(?:ed)?)\\b.*\\b(?:LINK|REFERENCE|URL|CITATION|SOURCE)s?\\b'\n",
    ")\n",
    "\n",
    "\n",
    "def extra_link_feats(df):\n",
    "    body = df[\"body\"].astype(str).fillna(\"\").map(normalize_text)\n",
    "    rule = df[\"rule\"].astype(str).fillna(\"\").map(normalize_text)\n",
    "\n",
    "    has_link_body = body.str.contains(r'\\bLINK\\b', regex=True, na=False).astype(np.float32)\n",
    "    link_count    = body.str.count(r'\\bLINK\\b', flags=re.I).astype(np.float32)\n",
    "    has_ref_body  = body.str.contains(r'\\bREFERENCE\\b', regex=True, na=False).astype(np.float32)\n",
    "\n",
    "    # note the non-capturing group and na=False\n",
    "    rule_forbids  = rule.str.contains(FORBID_RE, regex=True, na=False).astype(np.float32)\n",
    "    rule_mentions = rule.str.contains(r'\\b(?:LINK|REFERENCE)\\b', case=False, regex=True, na=False).astype(np.float32)\n",
    "\n",
    "    violates      = (has_link_body * rule_forbids).astype(np.float32)\n",
    "\n",
    "    return np.vstack([\n",
    "        has_link_body.values,\n",
    "        link_count.values,\n",
    "        has_ref_body.values,\n",
    "        rule_mentions.values,\n",
    "        rule_forbids.values,\n",
    "        violates.values,\n",
    "    ]).T\n",
    "\n",
    "\n",
    "# ---------- make train examples from original rows (no leakage) ----------\n",
    "def make_examples(df_rows: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for _, r in df_rows.iterrows():\n",
    "        rows += [\n",
    "            {\"body\": r[\"positive_example_1\"], \"rule\": r[\"rule\"], \"subreddit\": r[\"subreddit\"], \"label\": 1},\n",
    "            {\"body\": r[\"positive_example_2\"], \"rule\": r[\"rule\"], \"subreddit\": r[\"subreddit\"], \"label\": 1},\n",
    "            {\"body\": r[\"negative_example_1\"], \"rule\": r[\"rule\"], \"subreddit\": r[\"subreddit\"], \"label\": 0},\n",
    "            {\"body\": r[\"negative_example_2\"], \"rule\": r[\"rule\"], \"subreddit\": r[\"subreddit\"], \"label\": 0},\n",
    "        ]\n",
    "    return pd.DataFrame(rows)\n"
   ],
   "id": "94f092bcd4520036",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:29:14.196686Z",
     "start_time": "2025-08-17T14:29:14.137709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe_body = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        preprocessor=normalize_text,\n",
    "        ngram_range=(1,2), min_df=3, max_features=200_000,\n",
    "        lowercase=True, sublinear_tf=True, norm=None  # try norm=None for trees\n",
    "    )),\n",
    "    (\"svd\", TruncatedSVD(n_components=300, random_state=42, n_iter=7)),\n",
    "])\n",
    "\n",
    "# RULE: union(word, char) -> SVD (boosts vocab so k can be >= 64)\n",
    "rule_union = FeatureUnion([\n",
    "    (\"word\", TfidfVectorizer(\n",
    "        preprocessor=normalize_text,\n",
    "        ngram_range=(1,2), min_df=1, lowercase=True, sublinear_tf=True,\n",
    "        token_pattern=r\"(?u)\\b\\w+\\b\", norm=None\n",
    "    )),\n",
    "    (\"char\", TfidfVectorizer(\n",
    "        preprocessor=normalize_text,\n",
    "        analyzer=\"char_wb\", ngram_range=(3,6), min_df=1, sublinear_tf=True, norm=None\n",
    "    )),\n",
    "    # transformer_weights={\"word\":1.0, \"char\":0.8},  # optional downweight chars\n",
    "])\n",
    "\n",
    "pipe_rule = Pipeline([\n",
    "    (\"union\", rule_union),\n",
    "    (\"svd\", TruncatedSVD(n_components=128, random_state=42, n_iter=7)),\n",
    "])\n",
    "\n",
    "# pipe_sred = Pipeline([\n",
    "#     # Subreddit names are short; unigrams usually suffice\n",
    "#     (\"tfidf\", TfidfVectorizer(lowercase=False)),\n",
    "#     (\"svd\",   TruncatedSVD(n_components=16, random_state=42, n_iter=7)),\n",
    "#     # (\"norm\",  Normalizer(copy=False))\n",
    "# ])\n",
    "\n",
    "enc_sred = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n"
   ],
   "id": "93db74d0107621b0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:30:52.001145Z",
     "start_time": "2025-08-17T14:29:14.230920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "oof = np.zeros(len(df_trn), dtype=float)\n",
    "fold_test_preds = []\n",
    "fold_aucs = []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(df_trn, df_trn[\"rule_violation\"])):\n",
    "    tr_rows = df_trn.iloc[tr_idx].copy()\n",
    "    va_rows = df_trn.iloc[va_idx].copy()\n",
    "\n",
    "    trn_df = make_examples(tr_rows)\n",
    "    val_df = va_rows[[\"body\",\"rule\",\"subreddit\",\"rule_violation\"]].rename(columns={\"rule_violation\":\"label\"})\n",
    "    tst_df = df_tst[[\"body\",\"rule\",\"subreddit\"]].copy()\n",
    "\n",
    "    # Fit per-field on TRAIN only\n",
    "    Zb_trn = pipe_body.fit_transform(trn_df[\"body\"])\n",
    "    Zb_val = pipe_body.transform(val_df[\"body\"])\n",
    "    Zb_tst = pipe_body.transform(tst_df[\"body\"])\n",
    "\n",
    "    Zr_trn = pipe_rule.fit_transform(trn_df[\"rule\"])\n",
    "    Zr_val = pipe_rule.transform(val_df[\"rule\"])\n",
    "    Zr_tst = pipe_rule.transform(tst_df[\"rule\"])\n",
    "\n",
    "    Zs_trn = enc_sred.fit_transform(trn_df[[\"subreddit\"]])\n",
    "    Zs_val = enc_sred.transform(val_df[[\"subreddit\"]])\n",
    "    Zs_tst = enc_sred.transform(tst_df[[\"subreddit\"]])\n",
    "\n",
    "    # Optional: upweight RULE block slightly\n",
    "    rule_w = 1.3\n",
    "    Zr_trn *= rule_w; Zr_val *= rule_w; Zr_tst *= rule_w\n",
    "\n",
    "    # Extra numeric flags\n",
    "    F_trn = extra_link_feats(trn_df)\n",
    "    F_val = extra_link_feats(val_df)\n",
    "    F_tst = extra_link_feats(tst_df)\n",
    "\n",
    "    # Concatenate blocks (no global L2)\n",
    "    X_trn = np.hstack([Zb_trn, Zr_trn, Zs_trn, F_trn])\n",
    "    X_val = np.hstack([Zb_val, Zr_val, Zs_val, F_val])\n",
    "    X_tst = np.hstack([Zb_tst, Zr_tst, Zs_tst, F_tst])\n",
    "\n",
    "    y_trn = trn_df[\"label\"].values.astype(int)\n",
    "    y_val = val_df[\"label\"].values.astype(int)\n",
    "\n",
    "    # LightGBM (slightly regularized for dense SVD features)\n",
    "    params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"random_state\": 42,\n",
    "        \"n_estimators\": 4096,\n",
    "        \"learning_rate\": 0.012,\n",
    "        \"num_leaves\": 31,\n",
    "        \"min_child_samples\": 40,\n",
    "        \"lambda_l2\": 2.0,\n",
    "        \"feature_fraction\": 0.85,\n",
    "        \"bagging_fraction\": 0.9,\n",
    "        \"bagging_freq\": 1,\n",
    "        \"force_col_wise\": True,\n",
    "        # \"verbosity\": -1,\n",
    "    }\n",
    "    dtrn = lgb.Dataset(X_trn, label=y_trn)\n",
    "    dval = lgb.Dataset(X_val, label=y_val)\n",
    "    model = lgb.train(params, dtrn, num_boost_round=4096, valid_sets=[dval],\n",
    "                      callbacks=[lgb.early_stopping(stopping_rounds=64)])\n",
    "\n",
    "    # Store OOF preds mapped back to original rows (val fold has one row per original)\n",
    "    preds_val = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    oof[va_idx] = preds_val\n",
    "    fold_auc = roc_auc_score(y_val, preds_val)\n",
    "    fold_aucs.append(fold_auc)\n",
    "    print(f\"[fold {fold}] AUC: {fold_auc:.6f}  | best_iter: {model.best_iteration}\")\n",
    "\n",
    "    # Test predictions for this fold\n",
    "    fold_test_preds.append(model.predict(X_tst, num_iteration=model.best_iteration))\n",
    "\n",
    "# Overall CV\n",
    "cv_auc = roc_auc_score(df_trn[\"rule_violation\"].values.astype(int), oof)\n",
    "print(f\"CV AUC (10-fold): {cv_auc:.6f} | per-fold: {[round(a,6) for a in fold_aucs]}\")\n",
    "\n",
    "# Ensemble test preds\n",
    "pred_test = np.mean(fold_test_preds, axis=0)\n",
    "sub_df = pd.DataFrame({\"row_id\": df_tst[\"row_id\"], \"rule_violation\": pred_test})\n",
    "sub_df.to_csv(\"submission_cv_ens.csv\", index=False)\n",
    "print(\"submission_cv_ens.csv written; shape:\", sub_df.shape)\n",
    "print(sub_df.head(10))"
   ],
   "id": "61206a4736fbc517",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3652, number of negative: 3652\n",
      "[LightGBM] [Info] Total Bins 76793\n",
      "[LightGBM] [Info] Number of data points in the train set: 7304, number of used features: 475\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 64 rounds\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalid_0's auc: 0.921262\n",
      "[fold 0] AUC: 0.921262  | best_iter: 46\n",
      "[LightGBM] [Info] Number of positive: 3652, number of negative: 3652\n",
      "[LightGBM] [Info] Total Bins 76793\n",
      "[LightGBM] [Info] Number of data points in the train set: 7304, number of used features: 472\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 64 rounds\n",
      "Early stopping, best iteration is:\n",
      "[166]\tvalid_0's auc: 0.915146\n",
      "[fold 1] AUC: 0.915146  | best_iter: 166\n",
      "[LightGBM] [Info] Number of positive: 3652, number of negative: 3652\n",
      "[LightGBM] [Info] Total Bins 76803\n",
      "[LightGBM] [Info] Number of data points in the train set: 7304, number of used features: 474\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 64 rounds\n",
      "Early stopping, best iteration is:\n",
      "[650]\tvalid_0's auc: 0.87034\n",
      "[fold 2] AUC: 0.870340  | best_iter: 650\n",
      "[LightGBM] [Info] Number of positive: 3652, number of negative: 3652\n",
      "[LightGBM] [Info] Total Bins 76794\n",
      "[LightGBM] [Info] Number of data points in the train set: 7304, number of used features: 474\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 64 rounds\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalid_0's auc: 0.894078\n",
      "[fold 3] AUC: 0.894078  | best_iter: 395\n",
      "[LightGBM] [Info] Number of positive: 3652, number of negative: 3652\n",
      "[LightGBM] [Info] Total Bins 76778\n",
      "[LightGBM] [Info] Number of data points in the train set: 7304, number of used features: 473\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 64 rounds\n",
      "Early stopping, best iteration is:\n",
      "[505]\tvalid_0's auc: 0.90165\n",
      "[fold 4] AUC: 0.901650  | best_iter: 505\n",
      "[LightGBM] [Info] Number of positive: 3652, number of negative: 3652\n",
      "[LightGBM] [Info] Total Bins 76787\n",
      "[LightGBM] [Info] Number of data points in the train set: 7304, number of used features: 474\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 64 rounds\n",
      "Early stopping, best iteration is:\n",
      "[592]\tvalid_0's auc: 0.946893\n",
      "[fold 5] AUC: 0.946893  | best_iter: 592\n",
      "[LightGBM] [Info] Number of positive: 3652, number of negative: 3652\n",
      "[LightGBM] [Info] Total Bins 76796\n",
      "[LightGBM] [Info] Number of data points in the train set: 7304, number of used features: 475\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 64 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's auc: 0.892816\n",
      "[fold 6] AUC: 0.892816  | best_iter: 14\n",
      "[LightGBM] [Info] Number of positive: 3652, number of negative: 3652\n",
      "[LightGBM] [Info] Total Bins 76796\n",
      "[LightGBM] [Info] Number of data points in the train set: 7304, number of used features: 474\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 64 rounds\n",
      "Early stopping, best iteration is:\n",
      "[708]\tvalid_0's auc: 0.916117\n",
      "[fold 7] AUC: 0.916117  | best_iter: 708\n",
      "[LightGBM] [Info] Number of positive: 3652, number of negative: 3652\n",
      "[LightGBM] [Info] Total Bins 76791\n",
      "[LightGBM] [Info] Number of data points in the train set: 7304, number of used features: 474\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 64 rounds\n",
      "Early stopping, best iteration is:\n",
      "[353]\tvalid_0's auc: 0.921717\n",
      "[fold 8] AUC: 0.921717  | best_iter: 353\n",
      "[LightGBM] [Info] Number of positive: 3654, number of negative: 3654\n",
      "[LightGBM] [Info] Total Bins 76781\n",
      "[LightGBM] [Info] Number of data points in the train set: 7308, number of used features: 473\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 64 rounds\n",
      "Early stopping, best iteration is:\n",
      "[490]\tvalid_0's auc: 0.896832\n",
      "[fold 9] AUC: 0.896832  | best_iter: 490\n",
      "CV AUC (10-fold): 0.879902 | per-fold: [0.921262, 0.915146, 0.87034, 0.894078, 0.90165, 0.946893, 0.892816, 0.916117, 0.921717, 0.896832]\n",
      "submission_cv_ens.csv written; shape: (10, 2)\n",
      "   row_id  rule_violation\n",
      "0    2029        0.206999\n",
      "1    2030        0.435545\n",
      "2    2031        0.811989\n",
      "3    2032        0.537874\n",
      "4    2033        0.894723\n",
      "5    2034        0.137626\n",
      "6    2035        0.464354\n",
      "7    2036        0.109819\n",
      "8    2037        0.092329\n",
      "9    2038        0.815968\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
