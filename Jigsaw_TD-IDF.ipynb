{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-21T00:55:29.026801Z",
     "start_time": "2025-08-21T00:55:27.907445Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "trn = \"C:/Users/satra/Downloads/jigsaw-agile-community-rules/train.csv\"\n",
    "tst = \"C:/Users/satra/Downloads/jigsaw-agile-community-rules/test.csv\"\n",
    "df_trn = pd.read_csv(trn)\n",
    "df_tst = pd.read_csv(tst)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T00:55:29.377237Z",
     "start_time": "2025-08-21T00:55:29.212404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trn_rows = []\n",
    "for idx, row in df_trn.iterrows():\n",
    "  trn_rows.append({\n",
    "    'body': row['positive_example_1'],\n",
    "    'rule': row['rule'],\n",
    "    'subreddit': row['subreddit'],\n",
    "    'label': 1\n",
    "  })\n",
    "\n",
    "  trn_rows.append({\n",
    "    'body': row['positive_example_2'],\n",
    "    'rule': row['rule'],\n",
    "    'subreddit': row['subreddit'],\n",
    "    'label': 1\n",
    "  })\n",
    "\n",
    "  trn_rows.append({\n",
    "    'body': row['negative_example_1'],\n",
    "    'rule': row['rule'],\n",
    "    'subreddit': row['subreddit'],\n",
    "    'label': 0\n",
    "  })\n",
    "\n",
    "  trn_rows.append({\n",
    "    'body': row['negative_example_2'],\n",
    "    'rule': row['rule'],\n",
    "    'subreddit': row['subreddit'],\n",
    "    'label': 0\n",
    "  })\n",
    "\n",
    "trn_df = pd.DataFrame(trn_rows)\n",
    "\n",
    "val_rows = []\n",
    "for idx, row in df_trn.iterrows():\n",
    "  val_rows.append({\n",
    "    'body': row['body'],\n",
    "    'rule': row['rule'],\n",
    "    'subreddit': row['subreddit'],\n",
    "    'label': row['rule_violation']\n",
    "  })\n",
    "\n",
    "val_df = pd.DataFrame(val_rows)\n",
    "\n",
    "tst_rows = []\n",
    "for idx, row in df_tst.iterrows():\n",
    "  tst_rows.append({\n",
    "    'body': row['body'],\n",
    "    'rule': row['rule'],\n",
    "    'subreddit': row['subreddit']\n",
    "  })\n",
    "\n",
    "tst_df = pd.DataFrame(tst_rows)\n",
    "print (f'Train shape: {trn_df.shape}, Val shape: {val_df.shape}, Test shape: {tst_df.shape}')"
   ],
   "id": "1bddb764c85731c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (8116, 4), Val shape: (2029, 4), Test shape: (10, 3)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T00:55:29.472906Z",
     "start_time": "2025-08-21T00:55:29.414805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# ---------- text normalization (urls -> LINK, unify synonyms) ----------\n",
    "URL_RE     = re.compile(r'(?i)\\b(?:https?://|www\\.)[^\\s)]+')\n",
    "MD_LINK_RE = re.compile(r'\\[([^\\]]+)\\]\\((?:https?://|www\\.)[^\\s)]+\\)')\n",
    "\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        s = \"\" if s is None else str(s)\n",
    "    s = MD_LINK_RE.sub(r'\\1 LINK', s)\n",
    "    s = URL_RE.sub(' LINK ', s)\n",
    "    s = re.sub(r'(?i)\\burls?\\b', ' LINK ', s)\n",
    "    s = re.sub(r'(?i)\\blinks?\\b', ' LINK ', s)\n",
    "    s = re.sub(r'(?i)\\breferences?\\b', ' REFERENCE ', s)\n",
    "    s = re.sub(r'(?i)\\bcitations?\\b', ' REFERENCE ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def clean_text_col(s: pd.Series) -> pd.Series:\n",
    "    return s.astype(str).fillna(\"\").map(normalize_text)\n",
    "\n",
    "# ---------- extra high-signal flags (link in body vs rule forbids) ----------\n",
    "FORBID_RE = re.compile(\n",
    "    r'(?i)\\b(?:no|without|avoid|prohibit(?:ed)?)\\b.*\\b(?:LINK|REFERENCE|URL|CITATION|SOURCE)s?\\b'\n",
    ")\n",
    "\n",
    "\n",
    "def extra_link_feats(df):\n",
    "    body = df[\"body\"].astype(str).fillna(\"\").map(normalize_text)\n",
    "    rule = df[\"rule\"].astype(str).fillna(\"\").map(normalize_text)\n",
    "\n",
    "    has_link_body = body.str.contains(r'\\bLINK\\b', regex=True, na=False).astype(np.float32)\n",
    "    link_count    = body.str.count(r'\\bLINK\\b', flags=re.I).astype(np.float32)\n",
    "    has_ref_body  = body.str.contains(r'\\bREFERENCE\\b', regex=True, na=False).astype(np.float32)\n",
    "\n",
    "    # note the non-capturing group and na=False\n",
    "    rule_forbids  = rule.str.contains(FORBID_RE, regex=True, na=False).astype(np.float32)\n",
    "    rule_mentions = rule.str.contains(r'\\b(?:LINK|REFERENCE)\\b', case=False, regex=True, na=False).astype(np.float32)\n",
    "\n",
    "    violates      = (has_link_body * rule_forbids).astype(np.float32)\n",
    "\n",
    "    return np.vstack([\n",
    "        has_link_body.values,\n",
    "        link_count.values,\n",
    "        has_ref_body.values,\n",
    "        rule_mentions.values,\n",
    "        rule_forbids.values,\n",
    "        violates.values,\n",
    "    ]).T\n",
    "\n",
    "\n",
    "# ---------- make train examples from original rows (no leakage) ----------\n",
    "def make_examples(df_rows, include_orig=False, orig_weight=3.0):\n",
    "    rows, weights = [], []\n",
    "    for _, r in df_rows.iterrows():\n",
    "        # curated\n",
    "        rows += [\n",
    "          {\"body\": r[\"positive_example_1\"], \"rule\": r[\"rule\"], \"subreddit\": r[\"subreddit\"], \"label\": 1},\n",
    "          {\"body\": r[\"positive_example_2\"], \"rule\": r[\"rule\"], \"subreddit\": r[\"subreddit\"], \"label\": 1},\n",
    "          {\"body\": r[\"negative_example_1\"], \"rule\": r[\"rule\"], \"subreddit\": r[\"subreddit\"], \"label\": 0},\n",
    "          {\"body\": r[\"negative_example_2\"], \"rule\": r[\"rule\"], \"subreddit\": r[\"subreddit\"], \"label\": 0},\n",
    "        ]\n",
    "        weights += [1.0, 1.0, 1.0, 1.0]\n",
    "        # original body from the same row\n",
    "        if include_orig:\n",
    "            rows.append({\"body\": r[\"body\"], \"rule\": r[\"rule\"], \"subreddit\": r[\"subreddit\"],\n",
    "                         \"label\": int(r[\"rule_violation\"])})\n",
    "            weights.append(orig_weight)  # upweight to counter the 4 curated per row\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df, np.asarray(weights, dtype=np.float32)"
   ],
   "id": "94f092bcd4520036",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T00:55:29.598218Z",
     "start_time": "2025-08-21T00:55:29.491693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe_body = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        preprocessor=normalize_text,\n",
    "        ngram_range=(1,2), min_df=3, max_features=200_000,\n",
    "        lowercase=True, sublinear_tf=True, norm=None  # try norm=None for trees\n",
    "    )),\n",
    "    (\"svd\", TruncatedSVD(n_components=300, random_state=42, n_iter=7)),\n",
    "])\n",
    "\n",
    "# RULE: union(word, char) -> SVD (boosts vocab so k can be >= 64)\n",
    "rule_union = FeatureUnion([\n",
    "    (\"word\", TfidfVectorizer(\n",
    "        preprocessor=normalize_text,\n",
    "        ngram_range=(1,2), min_df=1, lowercase=True, sublinear_tf=True,\n",
    "        token_pattern=r\"(?u)\\b\\w+\\b\", norm=None\n",
    "    )),\n",
    "    (\"char\", TfidfVectorizer(\n",
    "        preprocessor=normalize_text,\n",
    "        analyzer=\"char_wb\", ngram_range=(3,6), min_df=1, sublinear_tf=True, norm=None\n",
    "    )),\n",
    "    # transformer_weights={\"word\":1.0, \"char\":0.8},  # optional downweight chars\n",
    "])\n",
    "\n",
    "pipe_rule = Pipeline([\n",
    "    (\"union\", rule_union),\n",
    "    (\"svd\", TruncatedSVD(n_components=128, random_state=42, n_iter=7)),\n",
    "])\n",
    "\n",
    "# pipe_sred = Pipeline([\n",
    "#     # Subreddit names are short; unigrams usually suffice\n",
    "#     (\"tfidf\", TfidfVectorizer(lowercase=False)),\n",
    "#     (\"svd\",   TruncatedSVD(n_components=16, random_state=42, n_iter=7)),\n",
    "#     # (\"norm\",  Normalizer(copy=False))\n",
    "# ])\n",
    "\n",
    "enc_sred = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n"
   ],
   "id": "93db74d0107621b0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T00:55:29.661059Z",
     "start_time": "2025-08-21T00:55:29.634874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==== RECC helpers: shared LSA + per-rule prototype bank ====\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "def fit_shared_lsa(corpus_texts, n_components=256, random_state=42):\n",
    "    vec = TfidfVectorizer(\n",
    "        preprocessor=normalize_text,\n",
    "        ngram_range=(1,2), min_df=2,\n",
    "        sublinear_tf=True, lowercase=False, norm=None\n",
    "    )\n",
    "    X = vec.fit_transform(corpus_texts)\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=random_state, n_iter=7)\n",
    "    Z = svd.fit_transform(X)\n",
    "    norm = Normalizer(copy=False)  # L2 so dot ~ cosine\n",
    "    norm.fit(Z)\n",
    "    return vec, svd, norm\n",
    "\n",
    "def lsa_embed(texts, vec, svd, norm):\n",
    "    X = vec.transform(texts)\n",
    "    Z = svd.transform(X)\n",
    "    return norm.transform(Z)\n",
    "\n",
    "def build_proto_bank(tr_rows):\n",
    "    \"\"\"\n",
    "    Build per-rule positive/negative 'prototype' texts from the TRAIN fold only.\n",
    "    Use normalized rule text as the key so matching is robust.\n",
    "    \"\"\"\n",
    "    bank = {}\n",
    "    for _, r in tr_rows.iterrows():\n",
    "        key = normalize_text(str(r[\"rule\"]))\n",
    "        b = bank.setdefault(key, {\"pos\": [], \"neg\": [], \"rule\": str(r[\"rule\"])})\n",
    "        # collect examples if present\n",
    "        for c in (\"positive_example_1\",\"positive_example_2\"):\n",
    "            if c in r and isinstance(r[c], str) and r[c].strip():\n",
    "                b[\"pos\"].append(r[c])\n",
    "        for c in (\"negative_example_1\",\"negative_example_2\"):\n",
    "            if c in r and isinstance(r[c], str) and r[c].strip():\n",
    "                b[\"neg\"].append(r[c])\n",
    "    return bank\n",
    "\n",
    "def preembed_bank(bank, vec, svd, norm):\n",
    "    \"\"\"\n",
    "    Pre-embed all rule texts and their prototypes once.\n",
    "    \"\"\"\n",
    "    bank_emb = {}\n",
    "    for key, d in bank.items():\n",
    "        Er = lsa_embed([d[\"rule\"]], vec, svd, norm)              # [1, d]\n",
    "        Epos = lsa_embed(d[\"pos\"], vec, svd, norm) if d[\"pos\"] else np.zeros((0, Er.shape[1]))\n",
    "        Eneg = lsa_embed(d[\"neg\"], vec, svd, norm) if d[\"neg\"] else np.zeros((0, Er.shape[1]))\n",
    "        bank_emb[key] = {\"Er\": Er, \"Epos\": Epos, \"Eneg\": Eneg}\n",
    "    return bank_emb\n",
    "\n",
    "def recc_block(df, bank_emb, vec, svd, norm, topk=2):\n",
    "    \"\"\"\n",
    "    Compute RECC features for each row using the bank from TRAIN fold.\n",
    "    For rows whose rule isn't in the bank, we fallback to empty Epos/Eneg and embed rule on the fly.\n",
    "    \"\"\"\n",
    "\n",
    "    # work on a copy with 0..N-1 index to keep positional alignment\n",
    "    df_ = df.reset_index(drop=True)\n",
    "\n",
    "    bodies_norm = [normalize_text(x) for x in df_[\"body\"].astype(str).fillna(\"\")]\n",
    "    Eb_all = lsa_embed(bodies_norm, vec, svd, norm)  # shape [N, d]\n",
    "\n",
    "    rule_cache = {}\n",
    "    feats = []\n",
    "\n",
    "    for j, row in df_.iterrows():        # j is 0..N-1\n",
    "        Eb = Eb_all[j:j+1]               # [1, d]\n",
    "        key = normalize_text(str(row[\"rule\"]))\n",
    "\n",
    "        if key in bank_emb:\n",
    "            Er   = bank_emb[key][\"Er\"]   # [1, d]\n",
    "            Epos = bank_emb[key][\"Epos\"] # [P, d]\n",
    "            Eneg = bank_emb[key][\"Eneg\"] # [N, d]\n",
    "        else:\n",
    "            if key not in rule_cache:\n",
    "                rule_cache[key] = lsa_embed([str(row[\"rule\"])], vec, svd, norm)\n",
    "            Er = rule_cache[key]         # [1, d]\n",
    "            Epos = np.zeros((0, Er.shape[1]))\n",
    "            Eneg = np.zeros((0, Er.shape[1]))\n",
    "\n",
    "        # cosines\n",
    "        s_br = float((Eb @ Er.T)[0, 0])\n",
    "\n",
    "        if Epos.shape[0]:\n",
    "            sims = (Eb @ Epos.T).ravel()\n",
    "            s_bp_pos_max  = float(sims.max())\n",
    "            k = min(topk, len(sims))\n",
    "            s_bp_pos_topk = float(np.mean(np.sort(sims)[-k:]))\n",
    "        else:\n",
    "            s_bp_pos_max = s_bp_pos_topk = -1.0\n",
    "\n",
    "        s_bp_neg_max = float((Eb @ Eneg.T).max()) if Eneg.shape[0] else -1.0\n",
    "        margin_pos_minus_neg = s_bp_pos_max - max(s_bp_neg_max, -1.0)\n",
    "\n",
    "        feats.append([s_br, s_bp_pos_max, s_bp_pos_topk, s_bp_neg_max, margin_pos_minus_neg])\n",
    "\n",
    "    cols = [\n",
    "        \"recc_cos_body_rule\",\n",
    "        \"recc_cos_body_pos_max\",\n",
    "        \"recc_cos_body_pos_topk\",\n",
    "        \"recc_cos_body_neg_max\",\n",
    "        \"recc_margin_pos_minus_neg\",\n",
    "    ]\n",
    "\n",
    "    return pd.DataFrame(feats, columns=cols, index=df.index)  # preserve original index\n"
   ],
   "id": "a8281eefaaed8f41",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T00:57:40.843964Z",
     "start_time": "2025-08-21T00:55:29.698972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "groups = pd.factorize(df_trn[\"subreddit\"].astype(str).str.lower().str.strip())[0]\n",
    "cv = StratifiedGroupKFold(n_splits=min(5, len(np.unique(groups))), shuffle=True, random_state=42)\n",
    "\n",
    "oof = np.zeros(len(df_trn), dtype=float)\n",
    "fold_test_preds = []\n",
    "fold_aucs = []\n",
    "\n",
    "# for fold, (tr_idx, va_idx) in enumerate(gkf.split(df_trn, df_trn[\"rule_violation\"], groups=df_trn[\"rule\"])):\n",
    "fold = 0\n",
    "for tr_idx, va_idx in cv.split(df_trn, df_trn[\"rule_violation\"], groups):\n",
    "    fold += 1\n",
    "    tr_rows, va_rows = df_trn.iloc[tr_idx], df_trn.iloc[va_idx]\n",
    "    trn_df, w_trn = make_examples(tr_rows, include_orig=True, orig_weight=3.0)\n",
    "\n",
    "    trn_df, w_trn = make_examples(tr_rows, include_orig=True, orig_weight=3.0)\n",
    "    val_df = va_rows[[\"body\",\"rule\",\"subreddit\",\"rule_violation\"]].rename(columns={\"rule_violation\":\"label\"})\n",
    "    tst_df = df_tst[[\"body\",\"rule\",\"subreddit\"]].copy()\n",
    "\n",
    "    # Fit per-field on TRAIN only\n",
    "    Zb_trn = pipe_body.fit_transform(trn_df[\"body\"])\n",
    "    Zb_val = pipe_body.transform(val_df[\"body\"])\n",
    "    Zb_tst = pipe_body.transform(tst_df[\"body\"])\n",
    "\n",
    "    Zr_trn = pipe_rule.fit_transform(trn_df[\"rule\"])\n",
    "    Zr_val = pipe_rule.transform(val_df[\"rule\"])\n",
    "    Zr_tst = pipe_rule.transform(tst_df[\"rule\"])\n",
    "\n",
    "    Zs_trn = enc_sred.fit_transform(trn_df[[\"subreddit\"]])\n",
    "    Zs_val = enc_sred.transform(val_df[[\"subreddit\"]])\n",
    "    Zs_tst = enc_sred.transform(tst_df[[\"subreddit\"]])\n",
    "\n",
    "    # Optional: upweight RULE block slightly\n",
    "    rule_w = 1.3\n",
    "    Zr_trn *= rule_w; Zr_val *= rule_w; Zr_tst *= rule_w\n",
    "\n",
    "    # Extra numeric flags\n",
    "    F_trn = extra_link_feats(trn_df)\n",
    "    F_val = extra_link_feats(val_df)\n",
    "    F_tst = extra_link_feats(tst_df)\n",
    "\n",
    "    # ... inside your CV loop, after you’ve built tr_rows, va_rows, trn_df/val_df/tst_df ...\n",
    "\n",
    "    # --- Build prototype bank from TRAIN-FOLD ONLY (original rows) ---\n",
    "    proto_bank = build_proto_bank(tr_rows)\n",
    "\n",
    "    # --- Fit shared LSA on TRAIN-FOLD ONLY ---\n",
    "    # Use a rich but leakage-safe corpus: train rules + all train prototypes + (optionally) train bodies\n",
    "    lsa_corpus = []\n",
    "    lsa_corpus += [normalize_text(x) for x in tr_rows[\"rule\"].astype(str)]\n",
    "    for d in proto_bank.values():\n",
    "        lsa_corpus += [normalize_text(x) for x in d[\"pos\"]]\n",
    "        lsa_corpus += [normalize_text(x) for x in d[\"neg\"]]\n",
    "    # (optional) also include the train bodies\n",
    "    lsa_corpus += [normalize_text(x) for x in tr_rows[\"body\"].astype(str)]\n",
    "\n",
    "    vec_lsa, svd_lsa, norm_lsa = fit_shared_lsa(lsa_corpus, n_components=256, random_state=42)\n",
    "\n",
    "    # --- Pre-embed the bank once ---\n",
    "    bank_emb = preembed_bank(proto_bank, vec_lsa, svd_lsa, norm_lsa)\n",
    "\n",
    "    # --- RECC blocks (dense) for this fold ---\n",
    "    R_trn = recc_block(trn_df, bank_emb, vec_lsa, svd_lsa, norm_lsa, topk=2)\n",
    "    R_val = recc_block(val_df, bank_emb, vec_lsa, svd_lsa, norm_lsa, topk=2)\n",
    "\n",
    "    # Test: you can only use what the bank knows. If a test rule isn't in the bank,\n",
    "    # it falls back to empty prototypes (features become conservative).\n",
    "    R_tst = recc_block(tst_df, bank_emb, vec_lsa, svd_lsa, norm_lsa, topk=2)\n",
    "\n",
    "    # --- Concatenate with your existing blocks ---\n",
    "    X_trn = np.hstack([Zb_trn, Zr_trn, Zs_trn, F_trn, R_trn.values])\n",
    "    X_val = np.hstack([Zb_val, Zr_val, Zs_val, F_val, R_val.values])\n",
    "    X_tst = np.hstack([Zb_tst, Zr_tst, Zs_tst, F_tst, R_tst.values])\n",
    "\n",
    "    # # Concatenate blocks (no global L2)\n",
    "    # X_trn = np.hstack([Zb_trn, Zr_trn, Zs_trn, F_trn])\n",
    "    # X_val = np.hstack([Zb_val, Zr_val, Zs_val, F_val])\n",
    "    # X_tst = np.hstack([Zb_tst, Zr_tst, Zs_tst, F_tst])\n",
    "\n",
    "    y_trn = trn_df[\"label\"].values.astype(int)\n",
    "    y_val = val_df[\"label\"].values.astype(int)\n",
    "\n",
    "    # LightGBM (slightly regularized for dense SVD features)\n",
    "    params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"random_state\": 42,\n",
    "        \"n_estimators\": 4096,\n",
    "        \"learning_rate\": 0.012,\n",
    "        \"num_leaves\": 31,\n",
    "        \"min_child_samples\": 40,\n",
    "        \"lambda_l2\": 2.0,\n",
    "        \"feature_fraction\": 0.85,\n",
    "        \"bagging_fraction\": 0.9,\n",
    "        \"bagging_freq\": 1,\n",
    "        \"force_col_wise\": True,\n",
    "        \"verbosity\": -1,\n",
    "    }\n",
    "\n",
    "    dtrn = lgb.Dataset(X_trn, label=trn_df[\"label\"].values, weight=w_trn)\n",
    "    dval = lgb.Dataset(X_val, label=val_df[\"label\"].values)\n",
    "    model = lgb.train(params, dtrn, num_boost_round=8192, valid_sets=[dtrn, dval], valid_names=[\"train\", \"val\"],\n",
    "                        callbacks=[lgb.early_stopping(stopping_rounds=256), lgb.log_evaluation(64)]\n",
    "    )\n",
    "\n",
    "    # Store OOF preds mapped back to original rows (val fold has one row per original)\n",
    "    preds_val = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    oof[va_idx] = preds_val\n",
    "    fold_auc = roc_auc_score(y_val, preds_val)\n",
    "    fold_aucs.append(fold_auc)\n",
    "    print(f\"[fold {fold}] AUC: {fold_auc:.6f}  | best_iter: {model.best_iteration}\")\n",
    "\n",
    "    # Test predictions for this fold\n",
    "    fold_test_preds.append(model.predict(X_tst, num_iteration=model.best_iteration, raw_score=True))\n",
    "\n",
    "# Overall CV\n",
    "cv_auc = roc_auc_score(df_trn[\"rule_violation\"].values.astype(int), oof)\n",
    "print(f\"CV AUC (10-fold): {cv_auc:.6f} | per-fold: {[round(a,6) for a in fold_aucs]}\")\n",
    "\n",
    "# Ensemble test preds\n",
    "pred_test = np.mean(fold_test_preds, axis=0)\n",
    "preds = 1.0 / (1.0 + np.exp(-pred_test))\n",
    "sub_df = pd.DataFrame({\"row_id\": df_tst[\"row_id\"], \"rule_violation\": preds})\n",
    "sub_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"submission.csv written; shape:\", sub_df.shape)\n",
    "print(sub_df.head(10))"
   ],
   "id": "61206a4736fbc517",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 256 rounds\n",
      "[64]\ttrain's auc: 0.995169\tval's auc: 0.904088\n",
      "[128]\ttrain's auc: 0.997599\tval's auc: 0.905112\n",
      "[192]\ttrain's auc: 0.998438\tval's auc: 0.902739\n",
      "[256]\ttrain's auc: 0.998718\tval's auc: 0.902142\n",
      "[320]\ttrain's auc: 0.998905\tval's auc: 0.902502\n",
      "Early stopping, best iteration is:\n",
      "[127]\ttrain's auc: 0.99759\tval's auc: 0.905174\n",
      "[fold 1] AUC: 0.905174  | best_iter: 127\n",
      "Training until validation scores don't improve for 256 rounds\n",
      "[64]\ttrain's auc: 0.995854\tval's auc: 0.954596\n",
      "[128]\ttrain's auc: 0.99692\tval's auc: 0.954188\n",
      "[192]\ttrain's auc: 0.99777\tval's auc: 0.953935\n",
      "[256]\ttrain's auc: 0.998409\tval's auc: 0.953799\n",
      "Early stopping, best iteration is:\n",
      "[13]\ttrain's auc: 0.993948\tval's auc: 0.956354\n",
      "[fold 2] AUC: 0.956354  | best_iter: 13\n",
      "Training until validation scores don't improve for 256 rounds\n",
      "[64]\ttrain's auc: 0.995971\tval's auc: 0.863557\n",
      "[128]\ttrain's auc: 0.99724\tval's auc: 0.862852\n",
      "[192]\ttrain's auc: 0.99812\tval's auc: 0.857895\n",
      "[256]\ttrain's auc: 0.998613\tval's auc: 0.855874\n",
      "Early stopping, best iteration is:\n",
      "[57]\ttrain's auc: 0.995926\tval's auc: 0.865249\n",
      "[fold 3] AUC: 0.865249  | best_iter: 57\n",
      "Training until validation scores don't improve for 256 rounds\n",
      "[64]\ttrain's auc: 0.99591\tval's auc: 0.913238\n",
      "[128]\ttrain's auc: 0.997394\tval's auc: 0.915789\n",
      "[192]\ttrain's auc: 0.998001\tval's auc: 0.921372\n",
      "[256]\ttrain's auc: 0.998492\tval's auc: 0.923405\n",
      "[320]\ttrain's auc: 0.998786\tval's auc: 0.925678\n",
      "[384]\ttrain's auc: 0.999173\tval's auc: 0.92488\n",
      "[448]\ttrain's auc: 0.999542\tval's auc: 0.924801\n",
      "[512]\ttrain's auc: 0.999758\tval's auc: 0.924482\n",
      "[576]\ttrain's auc: 0.999806\tval's auc: 0.926715\n",
      "[640]\ttrain's auc: 0.999827\tval's auc: 0.925917\n",
      "[704]\ttrain's auc: 0.999863\tval's auc: 0.927512\n",
      "[768]\ttrain's auc: 0.999893\tval's auc: 0.927592\n",
      "[832]\ttrain's auc: 0.999905\tval's auc: 0.92799\n",
      "[896]\ttrain's auc: 0.99991\tval's auc: 0.928708\n",
      "[960]\ttrain's auc: 0.999916\tval's auc: 0.929506\n",
      "[1024]\ttrain's auc: 0.999919\tval's auc: 0.929745\n",
      "[1088]\ttrain's auc: 0.999921\tval's auc: 0.929984\n",
      "[1152]\ttrain's auc: 0.999922\tval's auc: 0.929266\n",
      "[1216]\ttrain's auc: 0.999923\tval's auc: 0.930064\n",
      "[1280]\ttrain's auc: 0.999924\tval's auc: 0.930303\n",
      "[1344]\ttrain's auc: 0.999925\tval's auc: 0.930144\n",
      "[1408]\ttrain's auc: 0.999927\tval's auc: 0.929984\n",
      "[1472]\ttrain's auc: 0.999928\tval's auc: 0.930144\n",
      "Early stopping, best iteration is:\n",
      "[1242]\ttrain's auc: 0.999923\tval's auc: 0.930542\n",
      "[fold 4] AUC: 0.930542  | best_iter: 1242\n",
      "Training until validation scores don't improve for 256 rounds\n",
      "[64]\ttrain's auc: 0.99552\tval's auc: 0.900007\n",
      "[128]\ttrain's auc: 0.99721\tval's auc: 0.901396\n",
      "[192]\ttrain's auc: 0.997876\tval's auc: 0.902025\n",
      "[256]\ttrain's auc: 0.998495\tval's auc: 0.904125\n",
      "[320]\ttrain's auc: 0.998829\tval's auc: 0.90353\n",
      "[384]\ttrain's auc: 0.999189\tval's auc: 0.903034\n",
      "[448]\ttrain's auc: 0.999526\tval's auc: 0.904522\n",
      "[512]\ttrain's auc: 0.999698\tval's auc: 0.904291\n",
      "[576]\ttrain's auc: 0.999739\tval's auc: 0.903861\n",
      "[640]\ttrain's auc: 0.999785\tval's auc: 0.904853\n",
      "[704]\ttrain's auc: 0.999845\tval's auc: 0.903596\n",
      "[768]\ttrain's auc: 0.999888\tval's auc: 0.90353\n",
      "Early stopping, best iteration is:\n",
      "[542]\ttrain's auc: 0.999725\tval's auc: 0.904953\n",
      "[fold 5] AUC: 0.904953  | best_iter: 542\n",
      "CV AUC (10-fold): 0.889165 | per-fold: [0.905174, 0.956354, 0.865249, 0.930542, 0.904953]\n",
      "submission.csv written; shape: (10, 2)\n",
      "   row_id  rule_violation\n",
      "0    2029        0.190472\n",
      "1    2030        0.027820\n",
      "2    2031        0.966678\n",
      "3    2032        0.824387\n",
      "4    2033        0.974907\n",
      "5    2034        0.025444\n",
      "6    2035        0.835233\n",
      "7    2036        0.021196\n",
      "8    2037        0.153622\n",
      "9    2038        0.948887\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
