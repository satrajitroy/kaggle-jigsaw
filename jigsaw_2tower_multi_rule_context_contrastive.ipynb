{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"jigsaw_2tower_multi_rule_context_contrastive.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1nJNMkKf_9JMDQxDOgIHhrCpfw6eVjFe1\n",
    "\"\"\"\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "# for dirname, _, filenames in os.walk('.'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "# !pip install pytorch_metric_learning\n",
    "\n",
    "\n",
    "# ---- Imports\n",
    "import random, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Load and preprocess data\n",
    "# -----------------------------\n",
    "# Use Kaggle paths when running on Kaggle\n",
    "# MODEL_PATH = \"/kaggle/input/xlm-roberta-base-offline/xlm_roberta_base_offline\"\n",
    "# MODEL_PATH = \"C:/Users/satra/Downloads/xlm_roberta_base_offline\"\n",
    "MODEL_PATH = \"xlm-roberta-base\"\n",
    "\n",
    "\n",
    "# trn = \"/kaggle/input/jigsaw-agile-community-rules/train.csv\"\n",
    "# tst = \"/kaggle/input/jigsaw-agile-community-rules/test.csv\"\n",
    "# trn = \"/content/drive/MyDrive/Colab Notebooks/train.csv\"\n",
    "# tst = \"/content/drive/MyDrive/Colab Notebooks/test.csv\"\n",
    "trn = \"C:/Users/satra/Downloads/jigsaw-agile-community-rules/train.csv\"\n",
    "tst = \"C:/Users/satra/Downloads/jigsaw-agile-community-rules/test.csv\"\n",
    "\n",
    "df_trn = pd.read_csv(trn)\n",
    "df_tst = pd.read_csv(tst)\n",
    "\n",
    "\n",
    "def fill_empty_examples_pandas(df):\n",
    "    example_cols = ['positive_example_1', 'positive_example_2', 'negative_example_1', 'negative_example_2']\n",
    "    for col in example_cols:\n",
    "        df[col] = df[col].fillna('').astype(str)\n",
    "\n",
    "    df['positive_example_1'] = df['positive_example_1'].mask(df['positive_example_1'] == '', df['positive_example_2'])\n",
    "    df['positive_example_2'] = df['positive_example_2'].mask(df['positive_example_2'] == '', df['positive_example_1'])\n",
    "\n",
    "    df['negative_example_1'] = df['negative_example_1'].mask(df['negative_example_1'] == '', df['negative_example_2'])\n",
    "    df['negative_example_2'] = df['negative_example_2'].mask(df['negative_example_2'] == '', df['negative_example_1'])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_text(value):\n",
    "    return str(value) if pd.notna(value) else ''\n",
    "\n",
    "\n",
    "def extract_texts(row):\n",
    "    return {\n",
    "        \"body\": get_text(row[\"body\"]),\n",
    "        \"rule\": get_text(row[\"rule\"]),\n",
    "        \"subreddit\": get_text(row[\"subreddit\"]),\n",
    "        \"pos1\": f\"{get_text(row['positive_example_1'])}\",\n",
    "        \"pos2\": f\"{get_text(row['positive_example_2'])}\",\n",
    "        \"neg1\": f\"{get_text(row['negative_example_1'])}\",\n",
    "        \"neg2\": f\"{get_text(row['negative_example_2'])}\",\n",
    "    }\n",
    "\n",
    "df_trn = fill_empty_examples_pandas(df_trn)\n",
    "df_tst = fill_empty_examples_pandas(df_tst)\n",
    "\n",
    "df_trn[\"inputs\"] = df_trn.apply(extract_texts, axis=1)\n",
    "df_tst[\"inputs\"] = df_tst.apply(extract_texts, axis=1) # Apply to test data too\n",
    "\n",
    "N_EPOCHS = 8\n",
    "k_folds = 5\n",
    "\n",
    "def build_rule_context(row: pd.Series):\n",
    "    rule = str(row.get(\"rule\", \"\"))\n",
    "    pe = [x for x in [row.get(\"positive_example_1\",\"\"), row.get(\"positive_example_2\",\"\")] if x]\n",
    "    ne = [x for x in [row.get(\"negative_example_1\",\"\"), row.get(\"negative_example_2\",\"\")] if x]\n",
    "\n",
    "    pos_ctx = [f\"<RULE> {rule} <POS> {p}\" for p in pe]\n",
    "    neg_ctx = [f\"<RULE> {rule} <NEG> {n}\" for n in ne]\n",
    "\n",
    "    # keep a plain rule text if you want to add it later to the score as well:\n",
    "    rule_only = f\"<RULE> {rule}\"\n",
    "    return pos_ctx, neg_ctx, rule_only\n",
    "\n",
    "\n",
    "class RuleTripleDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Yields:\n",
    "      - anchor_* : tokenized BODY + SEP + RULE (paired)\n",
    "      - pos_ctx_*: [P, L]  up to 2 positive contexts: \"<RULE> r <POS> pe_i\"\n",
    "      - neg_ctx_*: [N, L]  up to 2 negative contexts: \"<RULE> r <NEG> ne_i\"\n",
    "      - pos_mask : [P]     1 if a pos ctx exists in that slot, else 0\n",
    "      - neg_mask : [N]     1 if a neg ctx exists in that slot, else 0\n",
    "      - label    : float32 (train/val only)\n",
    "    \"\"\"\n",
    "    def __init__(self, df, tokenizer, max_len=256, is_test=False, seed=42):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.is_test = is_test\n",
    "        self.sep = self.tokenizer.sep_token or \" </s> \"\n",
    "        self.P_MAX, self.N_MAX = 2, 2  # two positives, two negatives\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def _tok(self, texts):\n",
    "        return self.tokenizer(\n",
    "            texts,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        body = str(row[\"body\"])\n",
    "        rule = str(row[\"rule\"])\n",
    "\n",
    "        # 1) Anchor: BODY + SEP + RULE\n",
    "        anc_text = f\"<BODY> {body}\" + self.sep + f\"<RULE> {rule}\"\n",
    "        anc = self._tok(anc_text)\n",
    "        anchor_input_ids     = anc[\"input_ids\"].squeeze(0).to(torch.long)\n",
    "        anchor_attention_mask= anc[\"attention_mask\"].squeeze(0).to(torch.long)\n",
    "\n",
    "        # 2) Separate positive/negative contexts (0..2 each)\n",
    "        pos_ctx, neg_ctx, _ = build_rule_context(row)\n",
    "\n",
    "        # pad/truncate to fixed counts\n",
    "        pos_ctx = (pos_ctx + [\"\"]*self.P_MAX)[:self.P_MAX]\n",
    "        neg_ctx = (neg_ctx + [\"\"]*self.N_MAX)[:self.N_MAX]\n",
    "\n",
    "        tok_pos = self._tok(pos_ctx)   # [P, L]\n",
    "        tok_neg = self._tok(neg_ctx)   # [N, L]\n",
    "\n",
    "        # masks: 1 if non-empty string, else 0\n",
    "        pos_mask = torch.tensor([1 if s else 0 for s in pos_ctx], dtype=torch.float32)\n",
    "        neg_mask = torch.tensor([1 if s else 0 for s in neg_ctx], dtype=torch.float32)\n",
    "\n",
    "        item = {\n",
    "            \"anchor_input_ids\": anchor_input_ids,\n",
    "            \"anchor_attention_mask\": anchor_attention_mask,\n",
    "            \"pos_input_ids\": tok_pos[\"input_ids\"].to(torch.long),           # [P, L]\n",
    "            \"pos_attention_mask\": tok_pos[\"attention_mask\"].to(torch.long), # [P, L]\n",
    "            \"neg_input_ids\": tok_neg[\"input_ids\"].to(torch.long),           # [N, L]\n",
    "            \"neg_attention_mask\": tok_neg[\"attention_mask\"].to(torch.long), # [N, L]\n",
    "            \"pos_mask\": pos_mask, \"neg_mask\": neg_mask,\n",
    "        }\n",
    "        if not self.is_test:\n",
    "            item[\"label\"] = torch.tensor(float(row[\"rule_violation\"]), dtype=torch.float32)\n",
    "        return item\n",
    "\n",
    "\n",
    "def safe_normalize(x, eps=1e-6):\n",
    "    return x / x.norm(dim=-1, keepdim=True).clamp_min(eps)\n",
    "\n",
    "\n",
    "class DualEncoder(nn.Module):\n",
    "    def __init__(self, model_name, hidden_size=None, proj_dim=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        if hidden_size is None:\n",
    "            hidden_size = getattr(self.encoder.config, \"hidden_size\", 768)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.proj_pair = nn.Linear(hidden_size, proj_dim)\n",
    "        self.proj_cls  = nn.Linear(hidden_size, proj_dim)\n",
    "        self.cls_head  = nn.Linear(proj_dim, 1)\n",
    "\n",
    "    def _encode_hidden(self, input_ids, attention_mask):\n",
    "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls = out.last_hidden_state[:, 0]\n",
    "        return self.dropout(cls)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        anchor_input_ids, anchor_attention_mask,\n",
    "        pos_input_ids, pos_attention_mask, pos_mask,\n",
    "        neg_input_ids, neg_attention_mask, neg_mask,\n",
    "    ):\n",
    "        B = anchor_input_ids.size(0)\n",
    "\n",
    "        # Anchor\n",
    "        h_anchor = self._encode_hidden(anchor_input_ids, anchor_attention_mask)   # [B, H]\n",
    "        anchor_emb = nn.functional.normalize(self.proj_pair(h_anchor), dim=-1)    # [B, D]\n",
    "\n",
    "        # Pos contexts: flatten -> encode -> reshape -> masked mean pool\n",
    "        B, P, L = pos_input_ids.shape\n",
    "        pos_flat_ids = pos_input_ids.reshape(B * P, L).contiguous()\n",
    "        pos_flat_msk = pos_attention_mask.reshape(B * P, L).contiguous()\n",
    "        h_pos_flat   = self._encode_hidden(pos_flat_ids, pos_flat_msk)            # [B*P, H]\n",
    "        h_pos        = h_pos_flat.view(B, P, -1)                                  # [B, P, H]\n",
    "        pos_mask_exp = pos_mask.unsqueeze(-1)                                     # [B, P, 1]\n",
    "        h_pos = h_pos * pos_mask_exp\n",
    "        pos_den = pos_mask.sum(dim=1, keepdim=True).clamp_min(1.0)                # [B, 1]\n",
    "        h_pos_pool = (h_pos.sum(dim=1) / pos_den)                                 # [B, H]\n",
    "        pos_emb = nn.functional.normalize(self.proj_pair(h_pos_pool), dim=-1)     # [B, D]\n",
    "\n",
    "        # Neg contexts\n",
    "        B2, N, L2 = neg_input_ids.shape\n",
    "        assert B2 == B and L2 == L\n",
    "        neg_flat_ids = neg_input_ids.reshape(B * N, L).contiguous()\n",
    "        neg_flat_msk = neg_attention_mask.reshape(B * N, L).contiguous()\n",
    "        h_neg_flat   = self._encode_hidden(neg_flat_ids, neg_flat_msk)            # [B*N, H]\n",
    "        h_neg        = h_neg_flat.view(B, N, -1)                                  # [B, N, H]\n",
    "        neg_mask_exp = neg_mask.unsqueeze(-1)                                     # [B, N, 1]\n",
    "        h_neg = h_neg * neg_mask_exp\n",
    "        neg_den = neg_mask.sum(dim=1, keepdim=True).clamp_min(1.0)                # [B, 1]\n",
    "        h_neg_pool = (h_neg.sum(dim=1) / neg_den)                                 # [B, H]\n",
    "        neg_emb = nn.functional.normalize(self.proj_pair(h_neg_pool), dim=-1)     # [B, D]\n",
    "\n",
    "        anchor_emb = safe_normalize(self.proj_pair(h_anchor))\n",
    "        pos_emb    = safe_normalize(self.proj_pair(h_pos_pool))\n",
    "        neg_emb    = safe_normalize(self.proj_pair(h_neg_pool))\n",
    "\n",
    "        # Classification branch (on the anchor)\n",
    "        cls_feat = self.proj_cls(h_anchor)\n",
    "        logits   = self.cls_head(cls_feat).squeeze(-1)                            # [B]\n",
    "\n",
    "        return logits, anchor_emb, pos_emb, neg_emb\n",
    "\n",
    "# ================================\n",
    "# Hybrid Jigsaw ACR\n",
    "# ================================\n",
    "\n",
    "# ------------------\n",
    "# CONFIG (edit here)\n",
    "# ------------------\n",
    "@dataclass\n",
    "class CFG:\n",
    "    model_name: str = \"xlm-roberta-base\"\n",
    "    max_len: int = 256\n",
    "    num_negatives: int = 4               # K (must be constant)\n",
    "    batch_size: int = 16\n",
    "    epochs: int = 3\n",
    "    lr: float = 2e-5\n",
    "    wd: float = 0.01\n",
    "    warmup_ratio: float = 0.1\n",
    "    dropout: float = 0.1\n",
    "    proj_dim: int = 256\n",
    "    use_amp: bool = True\n",
    "    grad_clip: float = 1.0\n",
    "    pair_margin: float = 0.2\n",
    "    loss_alpha_cls: float = 0.6          # blend: alpha * BCE + (1-alpha) * pair\n",
    "    seed: int = 42\n",
    "    num_workers: int = 2\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "CFG = CFG()\n",
    "\n",
    "# ----------------\n",
    "# Reproducibility\n",
    "# ----------------\n",
    "def set_seed(s=42):\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "    torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(CFG.seed)\n",
    "\n",
    "# -------------\n",
    "# Tokenizer\n",
    "# -------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\n",
    "\n",
    "# -------------\n",
    "# Losses\n",
    "# -------------\n",
    "bce_loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def pairwise_margin_loss(anchor_emb, pos_emb, neg_emb, labels, margin=0.2):\n",
    "    \"\"\"\n",
    "    labels: [B] float or long in {0,1}\n",
    "    \"\"\"\n",
    "    # cosine since embeddings are L2-normalized\n",
    "    sim_pos = (anchor_emb * pos_emb).sum(dim=-1)   # [B]\n",
    "    sim_neg = (anchor_emb * neg_emb).sum(dim=-1)   # [B]\n",
    "\n",
    "    # map labels to {-1, +1}: y=1 -> +1, y=0 -> -1\n",
    "    ysign = (labels.float() * 2.0) - 1.0           # [B]\n",
    "\n",
    "    # for y=1:  sim_pos - sim_neg;  for y=0: -(sim_pos - sim_neg) = sim_neg - sim_pos\n",
    "    margin_term = ysign * (sim_pos - sim_neg)\n",
    "\n",
    "    return torch.relu(margin - margin_term).mean()\n",
    "\n",
    "# --------------------\n",
    "# Training / Evaluation\n",
    "# --------------------\n",
    "def make_loader(df, is_test=False):\n",
    "    ds = RuleTripleDataset(\n",
    "        df, tokenizer,\n",
    "        max_len=CFG.max_len,\n",
    "        is_test=is_test,\n",
    "        seed=CFG.seed\n",
    "    )\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=not is_test,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=(CFG.num_workers > 0) and (not is_test),\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "def run_one_epoch(model, loader, optimizer=None, scheduler=None, scaler=None):\n",
    "    is_train = optimizer is not None\n",
    "    model.train() if is_train else model.eval()\n",
    "\n",
    "    total_loss, nsteps = 0.0, 0\n",
    "    all_probs, all_labels, all_logits = [], [], []\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = {k: (v.to(CFG.device) if torch.is_tensor(v) else v) for k, v in batch.items()}\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(CFG.use_amp and is_train)):\n",
    "            logits, a_emb, p_emb, n_emb = model(\n",
    "                anchor_input_ids=batch[\"anchor_input_ids\"],\n",
    "                anchor_attention_mask=batch[\"anchor_attention_mask\"],\n",
    "                pos_input_ids=batch[\"pos_input_ids\"],\n",
    "                pos_attention_mask=batch[\"pos_attention_mask\"],\n",
    "                pos_mask=batch[\"pos_mask\"],\n",
    "                neg_input_ids=batch[\"neg_input_ids\"],\n",
    "                neg_attention_mask=batch[\"neg_attention_mask\"],\n",
    "                neg_mask=batch[\"neg_mask\"],\n",
    "            )\n",
    "\n",
    "            if \"label\" in batch:\n",
    "                loss_cls = bce_loss_fn(logits, batch[\"label\"])\n",
    "                # label-aware margin\n",
    "                loss_pair = pairwise_margin_loss(\n",
    "                    a_emb, p_emb, n_emb, batch[\"label\"], margin=CFG.pair_margin\n",
    "                )\n",
    "            else:\n",
    "                loss_cls = torch.zeros((), device=CFG.device)\n",
    "                loss_pair = torch.zeros((), device=CFG.device)\n",
    "\n",
    "            loss = CFG.loss_alpha_cls * loss_cls + (1 - CFG.loss_alpha_cls) * loss_pair\n",
    "            # print(f\"Loss: {loss}\")\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if CFG.use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                if CFG.grad_clip is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.grad_clip)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                if CFG.grad_clip is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.grad_clip)\n",
    "                optimizer.step()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        nsteps += 1\n",
    "\n",
    "        if \"label\" in batch:\n",
    "            all_logits.extend(logits.detach().cpu().numpy().tolist())\n",
    "            all_probs.extend(torch.sigmoid(logits).detach().cpu().numpy().tolist())\n",
    "            all_labels.extend(batch[\"label\"].detach().cpu().numpy().tolist())\n",
    "\n",
    "    avg_loss = total_loss / max(nsteps, 1)\n",
    "    auc = roc_auc_score(all_labels, all_probs) if all_labels else None\n",
    "    return avg_loss, auc, (np.array(all_probs), np.array(all_labels), np.array(all_logits))\n",
    "\n",
    "\n",
    "def train_hybrid(df_train, df_val, df_test=None):\n",
    "    model = DualEncoder(\n",
    "        model_name=CFG.model_name,\n",
    "        proj_dim=CFG.proj_dim,\n",
    "        dropout=CFG.dropout\n",
    "    ).to(CFG.device)\n",
    "\n",
    "    # Optimizer & Scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.wd)\n",
    "    steps_per_epoch = math.ceil(len(df_train) / CFG.batch_size)\n",
    "    num_train_steps = steps_per_epoch * CFG.epochs\n",
    "    num_warmup_steps = int(num_train_steps * CFG.warmup_ratio)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.use_amp)\n",
    "\n",
    "    # Loaders\n",
    "    train_loader = make_loader(df_train, is_test=False)\n",
    "    val_loader   = make_loader(df_val,   is_test=False)\n",
    "    test_loader  = make_loader(df_test,  is_test=True) if df_test is not None else None\n",
    "\n",
    "    best_auc, best_state = -1.0, None\n",
    "\n",
    "    for epoch in range(1, CFG.epochs + 1):\n",
    "        tr_loss, tr_auc, _ = run_one_epoch(model, train_loader, optimizer=optimizer, scaler=scaler, scheduler=scheduler)\n",
    "        val_loss, val_auc, _ = run_one_epoch(model, val_loader, optimizer=None, scaler=None, scheduler=None)\n",
    "        print(f\"Epoch {epoch:02d} | TrainLoss {tr_loss:.4f}\"\n",
    "              f\"{' | TrainAUC ' + f'{tr_auc:.4f}' if tr_auc is not None else ''}\"\n",
    "              f\" | ValLoss {val_loss:.4f} | ValAUC {val_auc:.4f}\")\n",
    "\n",
    "        if val_auc is not None and val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    print(f\"Best Val AUC: {best_auc:.4f}\" if best_auc >= 0 else \"No validation AUC computed.\")\n",
    "\n",
    "    # --- collect VAL predictions (logits + probs) for OOF ---\n",
    "    model.eval()\n",
    "    val_probs_list, val_logits_list, val_labels_list = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: (v.to(CFG.device) if torch.is_tensor(v) else v) for k, v in batch.items()}\n",
    "            logits, _, _, _ = model(\n",
    "                anchor_input_ids=batch[\"anchor_input_ids\"],\n",
    "                anchor_attention_mask=batch[\"anchor_attention_mask\"],\n",
    "                pos_input_ids=batch[\"pos_ctx_input_ids\"],\n",
    "                pos_attention_mask=batch[\"pos_ctx_attention_mask\"],\n",
    "                pos_mask=batch[\"pos_mask\"],\n",
    "                neg_input_ids=batch[\"neg_ctx_input_ids\"],\n",
    "                neg_attention_mask=batch[\"neg_ctx_attention_mask\"],\n",
    "                neg_mask=batch[\"neg_mask\"],\n",
    "            )\n",
    "            val_logits_list.extend(logits.cpu().numpy().tolist())\n",
    "            val_probs_list.extend(torch.sigmoid(logits).cpu().numpy().tolist())\n",
    "            val_labels_list.extend(batch[\"label\"].cpu().numpy().tolist())\n",
    "    val_logits = np.array(val_logits_list)\n",
    "    val_probs  = np.array(val_probs_list)\n",
    "    val_labels = np.array(val_labels_list)\n",
    "\n",
    "    # --- TEST predictions (if provided) ---\n",
    "    test_logits = test_probs = None\n",
    "    if test_loader is not None:\n",
    "        all_logits = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                batch = {k: (v.to(CFG.device) if torch.is_tensor(v) else v) for k, v in batch.items()}\n",
    "                logits, _, _, _ = model(\n",
    "                    anchor_input_ids=batch[\"anchor_input_ids\"],\n",
    "                    anchor_attention_mask=batch[\"anchor_attention_mask\"],\n",
    "                    pos_input_ids=batch[\"pos_ctx_input_ids\"],\n",
    "                    pos_attention_mask=batch[\"pos_ctx_attention_mask\"],\n",
    "                    pos_mask=batch[\"pos_mask\"],\n",
    "                    neg_input_ids=batch[\"neg_ctx_input_ids\"],\n",
    "                    neg_attention_mask=batch[\"neg_ctx_attention_mask\"],\n",
    "                    neg_mask=batch[\"neg_mask\"],\n",
    "                )\n",
    "                all_logits.extend(logits.cpu().numpy().tolist())\n",
    "        test_logits = np.array(all_logits)\n",
    "        test_probs  = 1.0 / (1.0 + np.exp(-test_logits))  # sigmoid\n",
    "\n",
    "    return model, val_probs, val_labels, val_logits, test_probs, test_logits\n",
    "\n",
    "\n",
    "def _logit(p, eps=1e-6):\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "    return np.log(p / (1 - p))\n",
    "\n",
    "def _sigmoid(x):  # numpy\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def kfold_train(df_trn, df_tst, n_splits=5, group_col=\"subreddit\", seeds=(42, 1337, 2029)):\n",
    "    y = (df_trn[\"rule_violation\"] > 0.5).astype(int).values\n",
    "    use_group = group_col in df_trn.columns and df_trn[group_col].nunique() >= n_splits\n",
    "\n",
    "    oof_logits_all = np.zeros(len(df_trn), dtype=float)\n",
    "    test_logits_seeds = []   # one ensembled test-logit vector per seed\n",
    "    fold_auc_seeds = []\n",
    "\n",
    "    for s_idx, seed in enumerate(seeds):\n",
    "        set_seed(seed)\n",
    "\n",
    "        if use_group:\n",
    "            groups = df_trn[group_col].astype(str).values\n",
    "            cv = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "            fold_iter = cv.split(df_trn, y, groups)\n",
    "            print(f\"\\n[seed {seed}] Using StratifiedGroupKFold by '{group_col}' ({df_trn[group_col].nunique()} groups).\")\n",
    "        else:\n",
    "            cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "            fold_iter = cv.split(df_trn, y)\n",
    "            print(f\"\\n[seed {seed}] Using StratifiedKFold.\")\n",
    "\n",
    "        oof_logits = np.zeros(len(df_trn), dtype=float)\n",
    "        fold_aucs = []\n",
    "        test_fold_logits = []\n",
    "\n",
    "        for fold, (tr_idx, va_idx) in enumerate(fold_iter):\n",
    "            print(f\"=== Seed {seed} | Fold {fold+1}/{n_splits} ===\")\n",
    "            tr_df = df_trn.iloc[tr_idx].reset_index(drop=True)\n",
    "            va_df = df_trn.iloc[va_idx].reset_index(drop=True)\n",
    "\n",
    "            # fresh seed per fold for dataloaders/shuffling\n",
    "            set_seed(seed + fold)\n",
    "\n",
    "            model, val_probs, val_labels, val_logits, test_probs, test_logits = train_hybrid(tr_df, va_df, df_tst)\n",
    "            fold_auc = roc_auc_score(val_labels, val_probs)\n",
    "            print(f\"[seed {seed} fold {fold}] Val AUC: {fold_auc:.6f}\")\n",
    "\n",
    "            oof_logits[va_idx] = val_logits\n",
    "            fold_aucs.append(fold_auc)\n",
    "            if test_logits is not None:\n",
    "                test_fold_logits.append(test_logits)\n",
    "\n",
    "        # fold-weighted logit ensembling for this seed\n",
    "        if test_fold_logits:\n",
    "            W = np.array(fold_aucs); W = W / (W.sum() + 1e-12)\n",
    "            test_logits_ens = np.average(np.vstack(test_fold_logits), axis=0, weights=W)\n",
    "            test_logits_seeds.append(test_logits_ens)\n",
    "        else:\n",
    "            test_logits_ens = None\n",
    "\n",
    "        # stash seed’s OOF logits\n",
    "        oof_logits_all += oof_logits\n",
    "        fold_auc_seeds.append(fold_aucs)\n",
    "\n",
    "        # report seed CV\n",
    "        cv_auc_seed = roc_auc_score(y, _sigmoid(oof_logits))\n",
    "        print(f\"[seed {seed}] CV AUC: {cv_auc_seed:.6f} | per-fold: {[round(a,6) for a in fold_aucs]}\")\n",
    "\n",
    "    # average OOF logits across seeds\n",
    "    oof_logits_all /= max(len(seeds), 1)\n",
    "    cv_auc = roc_auc_score(y, _sigmoid(oof_logits_all))\n",
    "    print(f\"\\nOverall CV AUC (seeds {len(seeds)} × folds {n_splits}): {cv_auc:.6f}\")\n",
    "\n",
    "    # average test logits across seeds (simple mean)\n",
    "    pred_test = None\n",
    "    if test_logits_seeds:\n",
    "        pred_test = _sigmoid(np.mean(np.vstack(test_logits_seeds), axis=0))\n",
    "\n",
    "    return _sigmoid(oof_logits_all), pred_test, fold_auc_seeds, cv_auc\n",
    "\n",
    "\n",
    "oof_probs, test_probs, fold_auc_seeds, cv_auc = kfold_train(\n",
    "    df_trn=df_trn,\n",
    "    df_tst=df_tst,\n",
    "    n_splits=5,           # cap to available groups if needed\n",
    "    group_col=\"subreddit\",\n",
    "    seeds=(42, 1337, 2029)\n",
    ")\n",
    "\n",
    "print(\"Final CV AUC:\", cv_auc)\n",
    "if test_probs is not None:\n",
    "    sub = pd.DataFrame({\"row_id\": df_tst[\"row_id\"], \"rule_violation\": test_probs})\n",
    "    sub.to_csv(\"submission_kfold.csv\", index=False)\n",
    "    print(\"Saved submission_kfold.csv\")\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "id": "fec7bba255bef957",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 13121456,
     "isSourceIdPinned": false,
     "sourceId": 94635,
     "sourceType": "competition"
    },
    {
     "datasetId": 7984956,
     "sourceId": 12636496,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7311.633087,
   "end_time": "2025-08-03T16:13:47.493920",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-03T14:11:55.860833",
   "version": "2.6.0"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "L4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
