{
 "cells": [
  {
   "cell_type": "code",
   "id": "5c2c86b6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-03T14:12:00.340282Z",
     "iopub.status.busy": "2025-08-03T14:12:00.339614Z",
     "iopub.status.idle": "2025-08-03T14:12:02.002071Z",
     "shell.execute_reply": "2025-08-03T14:12:02.000970Z"
    },
    "papermill": {
     "duration": 1.66713,
     "end_time": "2025-08-03T14:12:02.003545",
     "exception": false,
     "start_time": "2025-08-03T14:12:00.336415",
     "status": "completed"
    },
    "tags": [],
    "id": "5c2c86b6",
    "ExecuteTime": {
     "end_time": "2025-08-09T15:09:29.442368Z",
     "start_time": "2025-08-09T15:09:29.435157Z"
    }
   },
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "# for dirname, _, filenames in os.walk('.'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T15:09:34.889808Z",
     "start_time": "2025-08-09T15:09:29.588092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# !pip install pytorch_metric_learning\n",
    "\n",
    "\n",
    "# ---- Imports\n",
    "import random, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Load and preprocess data\n",
    "# -----------------------------\n",
    "# Use Kaggle paths when running on Kaggle\n",
    "# MODEL_PATH = \"/kaggle/input/xlm-roberta-base-offline/xlm_roberta_base_offline\"\n",
    "# MODEL_PATH = \"C:/Users/satra/Downloads/xlm_roberta_base_offline\"\n",
    "MODEL_PATH = \"xlm-roberta-base\"\n",
    "\n",
    "\n",
    "# trn = \"/kaggle/input/jigsaw-agile-community-rules/train.csv\"\n",
    "# tst = \"/kaggle/input/jigsaw-agile-community-rules/test.csv\"\n",
    "# trn = \"/content/drive/MyDrive/Colab Notebooks/train.csv\"\n",
    "# tst = \"/content/drive/MyDrive/Colab Notebooks/test.csv\"\n",
    "trn = \"C:/Users/satra/Downloads/jigsaw-agile-community-rules/train.csv\"\n",
    "tst = \"C:/Users/satra/Downloads/jigsaw-agile-community-rules/test.csv\"\n",
    "\n",
    "df_trn = pd.read_csv(trn)\n",
    "df_trn = df_trn.sample(frac=.01, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df_tst = pd.read_csv(tst)\n",
    "\n",
    "\n",
    "def fill_empty_examples_pandas(df):\n",
    "    example_cols = ['positive_example_1', 'positive_example_2', 'negative_example_1', 'negative_example_2']\n",
    "    for col in example_cols:\n",
    "        df[col] = df[col].fillna('').astype(str)\n",
    "\n",
    "    df['positive_example_1'] = df['positive_example_1'].mask(df['positive_example_1'] == '', df['positive_example_2'])\n",
    "    df['positive_example_2'] = df['positive_example_2'].mask(df['positive_example_2'] == '', df['positive_example_1'])\n",
    "\n",
    "    df['negative_example_1'] = df['negative_example_1'].mask(df['negative_example_1'] == '', df['negative_example_2'])\n",
    "    df['negative_example_2'] = df['negative_example_2'].mask(df['negative_example_2'] == '', df['negative_example_1'])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_text(value):\n",
    "    return str(value) if pd.notna(value) else ''\n",
    "\n",
    "\n",
    "def extract_texts(row):\n",
    "    return {\n",
    "        \"body\": get_text(row[\"body\"]),\n",
    "        \"rule\": get_text(row[\"rule\"]),\n",
    "        \"subreddit\": get_text(row[\"subreddit\"]),\n",
    "        \"pos1\": f\"{get_text(row['positive_example_1'])}\",\n",
    "        \"pos2\": f\"{get_text(row['positive_example_2'])}\",\n",
    "        \"neg1\": f\"{get_text(row['negative_example_1'])}\",\n",
    "        \"neg2\": f\"{get_text(row['negative_example_2'])}\",\n",
    "    }\n",
    "\n",
    "df_trn = fill_empty_examples_pandas(df_trn)\n",
    "df_tst = fill_empty_examples_pandas(df_tst)\n",
    "\n",
    "df_trn[\"inputs\"] = df_trn.apply(extract_texts, axis=1)\n",
    "df_tst[\"inputs\"] = df_tst.apply(extract_texts, axis=1) # Apply to test data too\n",
    "\n",
    "N_EPOCHS = 8\n",
    "k_folds = 5\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)"
   ],
   "id": "5bbfcf0c657653c8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satra\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-09T15:09:34.918985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ================================\n",
    "# Hybrid Jigsaw ACR\n",
    "# ================================\n",
    "\n",
    "# ------------------\n",
    "# CONFIG (edit here)\n",
    "# ------------------\n",
    "@dataclass\n",
    "class CFG:\n",
    "    model_name: str = \"xlm-roberta-base\"  # or \"distilroberta-base\", etc.\n",
    "    max_len: int = 256\n",
    "    num_negatives: int = 4               # K (must be constant)\n",
    "    batch_size: int = 16\n",
    "    epochs: int = 3\n",
    "    lr: float = 2e-5\n",
    "    wd: float = 0.01\n",
    "    warmup_ratio: float = 0.1\n",
    "    dropout: float = 0.1\n",
    "    proj_dim: int = 256\n",
    "    use_amp: bool = True\n",
    "    grad_clip: float = 1.0\n",
    "    pair_margin: float = 0.2\n",
    "    loss_alpha_cls: float = 0.6          # blend: alpha * BCE + (1-alpha) * pair\n",
    "    seed: int = 42\n",
    "    num_workers: int = 2\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "CFG = CFG()\n",
    "\n",
    "# ----------------\n",
    "# Reproducibility\n",
    "# ----------------\n",
    "def set_seed(s=42):\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "    torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(CFG.seed)\n",
    "\n",
    "# -------------\n",
    "# Tokenizer\n",
    "# -------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\n",
    "\n",
    "# ---------------------\n",
    "# Dataset (triples, K-)\n",
    "# ---------------------\n",
    "class RuleTripleDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Yields a dict with:\n",
    "      - anchor_* : tokenized body + SEP + rule (paired)\n",
    "      - pos_*    : tokenized rule (positive)\n",
    "      - neg_*    : tokenized K negative rules (tensor [K, L])\n",
    "      - label    : float32 in [0,1] (omitted if is_test=True)\n",
    "    Expects df with columns: 'body', 'rule', and (for train/val) 'rule_violation'\n",
    "    \"\"\"\n",
    "    def __init__(self, df, tokenizer, num_negatives=4, max_len=256, is_test=False, seed=42):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_negatives = num_negatives\n",
    "        self.max_len = max_len\n",
    "        self.is_test = is_test\n",
    "\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.all_rules = self.df[\"rule\"].astype(str).tolist()\n",
    "\n",
    "        n = len(self.df)\n",
    "        all_idx = np.arange(n)\n",
    "        self.neg_indices = []\n",
    "        for i in range(n):\n",
    "            pool = np.delete(all_idx, i)\n",
    "            replace = len(pool) < self.num_negatives\n",
    "            chosen = rng.choice(pool, size=self.num_negatives, replace=replace)\n",
    "            self.neg_indices.append(chosen.tolist())\n",
    "\n",
    "        # robust sep\n",
    "        self.sep = self.tokenizer.sep_token or \" </s> \"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _tok(self, texts):\n",
    "        # Supports single string or a list of strings\n",
    "        return self.tokenizer(\n",
    "            texts,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        body = str(row[\"body\"])\n",
    "        rule = str(row[\"rule\"])\n",
    "\n",
    "        # Anchor: body + SEP + rule\n",
    "        anc_text = body + self.sep + rule\n",
    "        anc = self._tok(anc_text)\n",
    "        anchor_input_ids = anc[\"input_ids\"].squeeze(0).to(torch.long)\n",
    "        anchor_attention_mask = anc[\"attention_mask\"].squeeze(0).to(torch.long)\n",
    "\n",
    "        # Positive rule text\n",
    "        pos = self._tok(rule)\n",
    "        pos_input_ids = pos[\"input_ids\"].squeeze(0).to(torch.long)\n",
    "        pos_attention_mask = pos[\"attention_mask\"].squeeze(0).to(torch.long)\n",
    "\n",
    "        # Negatives: K other rules\n",
    "        neg_rules = [self.all_rules[j] for j in self.neg_indices[idx]]\n",
    "        neg = self._tok(neg_rules)\n",
    "        neg_input_ids = neg[\"input_ids\"].to(torch.long)                 # [K, L]\n",
    "        neg_attention_mask = neg[\"attention_mask\"].to(torch.long)       # [K, L]\n",
    "\n",
    "        item = {\n",
    "            \"anchor_input_ids\": anchor_input_ids,\n",
    "            \"anchor_attention_mask\": anchor_attention_mask,\n",
    "            \"pos_input_ids\": pos_input_ids,\n",
    "            \"pos_attention_mask\": pos_attention_mask,\n",
    "            \"neg_input_ids\": neg_input_ids,\n",
    "            \"neg_attention_mask\": neg_attention_mask,\n",
    "        }\n",
    "        if not self.is_test:\n",
    "            label = float(row[\"rule_violation\"])\n",
    "            item[\"label\"] = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        print(f'Item: {item}')\n",
    "        return item\n",
    "\n",
    "# -------------\n",
    "# Model\n",
    "# -------------\n",
    "class DualEncoderHybrid(nn.Module):\n",
    "    \"\"\"\n",
    "    - Shared transformer encoder for both anchor and rules\n",
    "    - Two projection heads:\n",
    "        * proj_pair  -> normalized embeddings for pair/contrastive loss\n",
    "        * proj_cls   -> features for BCE head (unnormalized)\n",
    "    - Returns: logits, anchor_emb, pos_emb, neg_emb\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, hidden_size=None, proj_dim=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        if hidden_size is None:\n",
    "            # heuristic: most base models have 768 hidden size; pull from config\n",
    "            hidden_size = getattr(self.encoder.config, \"hidden_size\", 768)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.proj_pair = nn.Linear(hidden_size, proj_dim)\n",
    "        self.proj_cls  = nn.Linear(hidden_size, proj_dim)\n",
    "        self.cls_head  = nn.Linear(proj_dim, 1)\n",
    "\n",
    "    def _encode_hidden(self, input_ids, attention_mask):\n",
    "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls = out.last_hidden_state[:, 0]  # CLS\n",
    "        return self.dropout(cls)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        anchor_input_ids,\n",
    "        anchor_attention_mask,\n",
    "        pos_input_ids,\n",
    "        pos_attention_mask,\n",
    "        neg_input_ids,\n",
    "        neg_attention_mask,\n",
    "    ):\n",
    "        B = anchor_input_ids.size(0)\n",
    "        L = anchor_input_ids.size(1)\n",
    "        K = neg_input_ids.size(0) // B if neg_input_ids.dim() == 2 else neg_input_ids.size(1)\n",
    "\n",
    "        # Anchor & positive\n",
    "        h_anchor = self._encode_hidden(anchor_input_ids, anchor_attention_mask)            # [B, H]\n",
    "        h_pos    = self._encode_hidden(pos_input_ids, pos_attention_mask)                  # [B, H]\n",
    "\n",
    "        # Negatives: flatten then reshape\n",
    "        if neg_input_ids.dim() == 3:\n",
    "            h_neg = self._encode_hidden(\n",
    "                neg_input_ids.view(B*K, -1),\n",
    "                neg_attention_mask.view(B*K, -1)\n",
    "            ).view(B, K, -1)                                                               # [B, K, H]\n",
    "        else:\n",
    "            raise ValueError(\"neg_input_ids must be [B, K, L]\")\n",
    "\n",
    "        # Pair embeddings (cosine)\n",
    "        anchor_emb = nn.functional.normalize(self.proj_pair(h_anchor), dim=-1)             # [B, D]\n",
    "        pos_emb    = nn.functional.normalize(self.proj_pair(h_pos), dim=-1)                # [B, D]\n",
    "        neg_emb    = nn.functional.normalize(self.proj_pair(h_neg), dim=-1)                # [B, K, D]\n",
    "\n",
    "        # Classification branch\n",
    "        cls_feat = self.proj_cls(h_anchor)\n",
    "        logits = self.cls_head(cls_feat).squeeze(-1)                                       # [B]\n",
    "        return logits, anchor_emb, pos_emb, neg_emb\n",
    "\n",
    "# -------------\n",
    "# Losses\n",
    "# -------------\n",
    "bce_loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def pairwise_margin_loss(anchor_emb, pos_emb, neg_emb, margin=0.2):\n",
    "    \"\"\"\n",
    "    Max(0, margin + sim(a, neg) - sim(a, pos)), averaged over batch and negatives.\n",
    "    Assumes normalized embeddings -> dot product = cosine similarity.\n",
    "    \"\"\"\n",
    "    # [B]\n",
    "    pos_sim = torch.sum(anchor_emb * pos_emb, dim=-1)\n",
    "    # [B, K]\n",
    "    neg_sim = torch.sum(anchor_emb.unsqueeze(1) * neg_emb, dim=-1)\n",
    "    loss = torch.relu(margin + neg_sim - pos_sim.unsqueeze(1))\n",
    "    return loss.mean()\n",
    "\n",
    "# --------------------\n",
    "# Training / Evaluation\n",
    "# --------------------\n",
    "def make_loader(df, is_test=False):\n",
    "    ds = RuleTripleDataset(\n",
    "        df, tokenizer,\n",
    "        num_negatives=CFG.num_negatives,\n",
    "        max_len=CFG.max_len,\n",
    "        is_test=is_test,\n",
    "        seed=CFG.seed\n",
    "    )\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=not is_test,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=(CFG.num_workers > 0) and (not is_test),\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "def run_one_epoch(model, loader, optimizer=None, scaler=None):\n",
    "    is_train = optimizer is not None\n",
    "    model.train() if is_train else model.eval()\n",
    "\n",
    "    total_loss, nsteps = 0.0, 0\n",
    "    all_probs, all_labels = [], []\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = {k: v.to(CFG.device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(CFG.use_amp and is_train)):\n",
    "            logits, a_emb, p_emb, n_emb = model(\n",
    "                batch[\"anchor_input_ids\"], batch[\"anchor_attention_mask\"],\n",
    "                batch[\"pos_input_ids\"],    batch[\"pos_attention_mask\"],\n",
    "                batch[\"neg_input_ids\"],    batch[\"neg_attention_mask\"],\n",
    "            )\n",
    "\n",
    "            if \"label\" in batch:\n",
    "                loss_cls = bce_loss_fn(logits, batch[\"label\"])\n",
    "            else:\n",
    "                # test-time: no labels; only forward pass\n",
    "                loss_cls = torch.zeros((), device=CFG.device)\n",
    "\n",
    "            loss_pair = pairwise_margin_loss(a_emb, p_emb, n_emb, margin=CFG.pair_margin)\n",
    "            loss = CFG.loss_alpha_cls * loss_cls + (1 - CFG.loss_alpha_cls) * loss_pair\n",
    "            print(f\"Loss: {loss}\")\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if CFG.use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                if CFG.grad_clip is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.grad_clip)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                if CFG.grad_clip is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.grad_clip)\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        nsteps += 1\n",
    "\n",
    "        # metrics buffer (only if labels exist)\n",
    "        if \"label\" in batch:\n",
    "            probs = torch.sigmoid(logits).detach().cpu().numpy().tolist()\n",
    "            labels = batch[\"label\"].detach().cpu().numpy().tolist()\n",
    "            all_probs.extend(probs)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    avg_loss = total_loss / max(nsteps, 1)\n",
    "    auc = roc_auc_score(all_labels, all_probs) if all_labels else None\n",
    "    return avg_loss, auc, (all_probs, all_labels)\n",
    "\n",
    "def train_hybrid(df_train, df_val, df_test=None):\n",
    "    model = DualEncoderHybrid(\n",
    "        model_name=CFG.model_name,\n",
    "        proj_dim=CFG.proj_dim,\n",
    "        dropout=CFG.dropout\n",
    "    ).to(CFG.device)\n",
    "\n",
    "    # Optimizer & Scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.wd)\n",
    "    steps_per_epoch = math.ceil(len(df_train) / CFG.batch_size)\n",
    "    num_train_steps = steps_per_epoch * CFG.epochs\n",
    "    num_warmup_steps = int(num_train_steps * CFG.warmup_ratio)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.use_amp)\n",
    "\n",
    "    # Loaders\n",
    "    train_loader = make_loader(df_train, is_test=False)\n",
    "    val_loader   = make_loader(df_val,   is_test=False)\n",
    "    test_loader  = make_loader(df_test,  is_test=True) if df_test is not None else None\n",
    "\n",
    "    best_auc, best_state = -1.0, None\n",
    "\n",
    "    for epoch in range(1, CFG.epochs + 1):\n",
    "        tr_loss, tr_auc, _ = run_one_epoch(model, train_loader, optimizer=optimizer, scaler=scaler)\n",
    "        scheduler.step()\n",
    "\n",
    "        val_loss, val_auc, _ = run_one_epoch(model, val_loader, optimizer=None, scaler=None)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | TrainLoss {tr_loss:.4f}\"\n",
    "              f\"{' | TrainAUC ' + f'{tr_auc:.4f}' if tr_auc is not None else ''}\"\n",
    "              f\" | ValLoss {val_loss:.4f} | ValAUC {val_auc:.4f}\")\n",
    "\n",
    "        if val_auc is not None and val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    print(f\"Best Val AUC: {best_auc:.4f}\" if best_auc >= 0 else \"No validation AUC computed.\")\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # Test inference\n",
    "    test_probs = None\n",
    "    if test_loader is not None:\n",
    "        model.eval()\n",
    "        all_probs = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                batch = {k: v.to(CFG.device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "                logits, _, _, _ = model(\n",
    "                    batch[\"anchor_input_ids\"], batch[\"anchor_attention_mask\"],\n",
    "                    batch[\"pos_input_ids\"],    batch[\"pos_attention_mask\"],\n",
    "                    batch[\"neg_input_ids\"],    batch[\"neg_attention_mask\"],\n",
    "                )\n",
    "                probs = torch.sigmoid(logits).detach().cpu().numpy().tolist()\n",
    "                all_probs.extend(probs)\n",
    "        test_probs = np.array(all_probs)\n",
    "\n",
    "    return model, test_probs\n",
    "\n",
    "# --------------------------\n",
    "# Convenience: data splitting\n",
    "# --------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_train_val(df, val_size=0.1, seed=42):\n",
    "    strat = (df[\"rule_violation\"] > 0.5).astype(int) if \"rule_violation\" in df.columns else None\n",
    "    tr, va = train_test_split(df, test_size=val_size, random_state=seed, stratify=strat)\n",
    "    return tr.reset_index(drop=True), va.reset_index(drop=True)\n",
    "\n",
    "# --------------------------\n",
    "# Quick Sanity Usage Example\n",
    "# --------------------------\n",
    "# Expect df_train to have columns: \"body\", \"rule\", \"rule_violation\"\n",
    "# Expect df_test  to have columns: \"body\", \"rule\" (no label)\n",
    "#\n",
    "train_df, val_df = split_train_val(df_trn, val_size=0.1, seed=CFG.seed)\n",
    "model, test_probs = train_hybrid(train_df, val_df, df_tst)\n",
    "#\n",
    "# # If you need a submission:\n",
    "sub = pd.DataFrame({\"prediction\": test_probs})\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Saved submission.csv\")\n"
   ],
   "id": "949c5722697148bd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satra\\AppData\\Local\\Temp\\ipykernel_5472\\1262751919.py:305: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=CFG.use_amp)\n",
      "C:\\Users\\satra\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\amp\\grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "C:\\Users\\satra\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "HJiik1MiRHr4"
   },
   "id": "HJiik1MiRHr4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 13121456,
     "isSourceIdPinned": false,
     "sourceId": 94635,
     "sourceType": "competition"
    },
    {
     "datasetId": 7984956,
     "sourceId": 12636496,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7311.633087,
   "end_time": "2025-08-03T16:13:47.493920",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-03T14:11:55.860833",
   "version": "2.6.0"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "L4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
