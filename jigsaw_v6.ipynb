{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5852915a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-01T02:27:48.967241Z",
     "iopub.status.busy": "2025-08-01T02:27:48.966862Z",
     "iopub.status.idle": "2025-08-01T02:27:50.639564Z",
     "shell.execute_reply": "2025-08-01T02:27:50.638511Z"
    },
    "papermill": {
     "duration": 1.679466,
     "end_time": "2025-08-01T02:27:50.640912",
     "exception": false,
     "start_time": "2025-08-01T02:27:48.961446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/xlm-roberta-base-offline/xlm_roberta_base_offline/config.json\n",
      "/kaggle/input/xlm-roberta-base-offline/xlm_roberta_base_offline/tokenizer_config.json\n",
      "/kaggle/input/xlm-roberta-base-offline/xlm_roberta_base_offline/model.safetensors\n",
      "/kaggle/input/xlm-roberta-base-offline/xlm_roberta_base_offline/special_tokens_map.json\n",
      "/kaggle/input/xlm-roberta-base-offline/xlm_roberta_base_offline/sentencepiece.bpe.model\n",
      "/kaggle/input/jigsaw-agile-community-rules/sample_submission.csv\n",
      "/kaggle/input/jigsaw-agile-community-rules/train.csv\n",
      "/kaggle/input/jigsaw-agile-community-rules/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3df21cc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T02:27:50.649712Z",
     "iopub.status.busy": "2025-08-01T02:27:50.648947Z",
     "iopub.status.idle": "2025-08-01T02:27:59.912796Z",
     "shell.execute_reply": "2025-08-01T02:27:59.911872Z"
    },
    "papermill": {
     "duration": 9.269795,
     "end_time": "2025-08-01T02:27:59.914553",
     "exception": false,
     "start_time": "2025-08-01T02:27:50.644758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All imports needed\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, XLMRobertaTokenizer #DebertaV2Tokenizer # AutoTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from tqdm import tqdm # For progress bars in console\n",
    "\n",
    "import google.protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64227a86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T02:27:59.922712Z",
     "iopub.status.busy": "2025-08-01T02:27:59.921982Z",
     "iopub.status.idle": "2025-08-01T02:27:59.926875Z",
     "shell.execute_reply": "2025-08-01T02:27:59.926132Z"
    },
    "papermill": {
     "duration": 0.009998,
     "end_time": "2025-08-01T02:27:59.928156",
     "exception": false,
     "start_time": "2025-08-01T02:27:59.918158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cleans and tokenizes text, preserving non-English words and symbols.\n",
    "def clean_and_tokenize_multilingual(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    # 1. Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Add spaces around any character that is NOT a letter, number, or whitespace.\n",
    "    # This isolates punctuation, emojis, and other symbols as separate tokens.\n",
    "    # For example, \"I hate you!!!\" becomes \" i hate you !!! \"\n",
    "    text = re.sub(r'([^a-zA-Z0-9\\s])', r' \\1 ', text)\n",
    "\n",
    "    # 3. Split text into words (tokenize)\n",
    "    words = text.split()\n",
    "\n",
    "    # 4. Remove English stop words and short words\n",
    "    # We keep non-English words because they are not in the stop_words list.\n",
    "    keywords = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0db9d4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T02:27:59.935729Z",
     "iopub.status.busy": "2025-08-01T02:27:59.935467Z",
     "iopub.status.idle": "2025-08-01T02:27:59.939023Z",
     "shell.execute_reply": "2025-08-01T02:27:59.938392Z"
    },
    "papermill": {
     "duration": 0.008861,
     "end_time": "2025-08-01T02:27:59.940369",
     "exception": false,
     "start_time": "2025-08-01T02:27:59.931508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a set of stop words for faster lookup (ensure this is defined once)\n",
    "stop_words = set(ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63646e7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T02:27:59.948295Z",
     "iopub.status.busy": "2025-08-01T02:27:59.948026Z",
     "iopub.status.idle": "2025-08-01T02:27:59.954133Z",
     "shell.execute_reply": "2025-08-01T02:27:59.953384Z"
    },
    "papermill": {
     "duration": 0.011483,
     "end_time": "2025-08-01T02:27:59.955468",
     "exception": false,
     "start_time": "2025-08-01T02:27:59.943985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_and_tokenize_with_urls(text):\n",
    "    \"\"\"\n",
    "    Cleans and tokenizes text, preserving non-English words and symbols,\n",
    "    and treating full URLs as single keywords.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    # 1. Find and extract URLs\n",
    "    # This regex captures http(s):// followed by non-whitespace, or www. followed by non-whitespace\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    urls = re.findall(url_pattern, text)\n",
    "\n",
    "    # 2. Replace URLs with a temporary placeholder to prevent them from being broken up\n",
    "    # Use a unique placeholder that won't naturally occur in text\n",
    "    placeholder = \"__URL_PLACEHOLDER__\"\n",
    "    text_without_urls = re.sub(url_pattern, placeholder, text)\n",
    "\n",
    "    # 3. Proceed with existing cleaning logic on the text without URLs\n",
    "    # Lowercase the text\n",
    "    text_without_urls = text_without_urls.lower()\n",
    "\n",
    "    # Add spaces around any character that is NOT a letter, number, or whitespace.\n",
    "    # This isolates punctuation, emojis, and other symbols as separate tokens.\n",
    "    text_without_urls = re.sub(r'([^a-zA-Z0-9\\s])', r' \\1 ', text_without_urls)\n",
    "\n",
    "    # Split text into words (tokenize)\n",
    "    words = text_without_urls.split()\n",
    "\n",
    "    # 4. Remove English stop words and short words, and remove the placeholder\n",
    "    # We keep non-English words because they are not in the stop_words list.\n",
    "    keywords = [word for word in words if word not in stop_words and len(word) > 1 and word != placeholder.lower()]\n",
    "\n",
    "    # 5. Add the extracted URLs to the keywords list\n",
    "    keywords.extend(urls)\n",
    "\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61e49f2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T02:27:59.963393Z",
     "iopub.status.busy": "2025-08-01T02:27:59.963125Z",
     "iopub.status.idle": "2025-08-01T02:27:59.967485Z",
     "shell.execute_reply": "2025-08-01T02:27:59.966747Z"
    },
    "papermill": {
     "duration": 0.009911,
     "end_time": "2025-08-01T02:27:59.968901",
     "exception": false,
     "start_time": "2025-08-01T02:27:59.958990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def violations_subreddit():\n",
    "    # Which subreddits have the highest average violation scores?\n",
    "    subreddit_violation = df_train.groupby('subreddit')['rule_violation'].mean().sort_values(ascending=False)\n",
    "    print(\"\\n--- Subreddits with Highest Average Violation Score ---\")\n",
    "    print(subreddit_violation.head(10))\n",
    "    print(\"\\n--- Subreddits with Lowest Average Violation Score ---\")\n",
    "    print(subreddit_violation.tail(10))\n",
    "    # See if length correlates with violation score\n",
    "    print(df_train[['comment_length', 'rule_violation']].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95da0f5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T02:27:59.976689Z",
     "iopub.status.busy": "2025-08-01T02:27:59.976478Z",
     "iopub.status.idle": "2025-08-01T02:27:59.981290Z",
     "shell.execute_reply": "2025-08-01T02:27:59.980543Z"
    },
    "papermill": {
     "duration": 0.010258,
     "end_time": "2025-08-01T02:27:59.982614",
     "exception": false,
     "start_time": "2025-08-01T02:27:59.972356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prelim_explore():\n",
    "    print(df_train.info())\n",
    "    print(df_train.describe())\n",
    "    # Set pandas to display the full column width\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    # Look at a few comments with HIGH violation scores\n",
    "    print(\"--- High Violation Comments ---\")\n",
    "    print(df_train[df_train['rule_violation'] > 0.8].head(3))\n",
    "    # Look at a few comments with LOW violation scores\n",
    "    print(\"\\n--- Low Violation Comments ---\")\n",
    "    print(df_train[df_train['rule_violation'] < 0.2].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15365d21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T02:27:59.990420Z",
     "iopub.status.busy": "2025-08-01T02:27:59.990148Z",
     "iopub.status.idle": "2025-08-01T02:27:59.995208Z",
     "shell.execute_reply": "2025-08-01T02:27:59.994654Z"
    },
    "papermill": {
     "duration": 0.010175,
     "end_time": "2025-08-01T02:27:59.996348",
     "exception": false,
     "start_time": "2025-08-01T02:27:59.986173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_keywords():\n",
    "    global significant_words\n",
    "    # This is the key step: transform the data to one-row-per-word\n",
    "    # This will create a much larger DataFrame\n",
    "    word_df = df_train.explode('keywords')\n",
    "    print(f\"Shape before exploding: {df_train.shape}\")\n",
    "    print(f\"Shape after exploding: {word_df.shape}\")\n",
    "    # Now, group by each keyword and calculate its stats\n",
    "    print(\"\\nCalculating word scores... (This may take a minute)\")\n",
    "    word_scores = word_df.groupby('keywords')['rule_violation'].agg(['count', 'mean']).reset_index()\n",
    "    word_scores.rename(columns={'mean': 'mean_violation'}, inplace=True)\n",
    "    print(\"Calculation complete.\")\n",
    "    # --- Filter out rare words to get more meaningful results ---\n",
    "    # A word needs to appear at least 50 times to be considered.\n",
    "    # This avoids drawing conclusions from words that only appear a few times.\n",
    "    min_word_count = 64\n",
    "    significant_words = word_scores[word_scores['count'] >= min_word_count]\n",
    "    print(f\"Total unique words: {word_scores.shape[0]}\")\n",
    "    print(f\"Significant words (>= {min_word_count} occurrences): {significant_words.shape[0]}\")\n",
    "    return significant_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64092da8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T02:28:00.004406Z",
     "iopub.status.busy": "2025-08-01T02:28:00.003974Z",
     "iopub.status.idle": "2025-08-01T02:28:00.008651Z",
     "shell.execute_reply": "2025-08-01T02:28:00.007914Z"
    },
    "papermill": {
     "duration": 0.00999,
     "end_time": "2025-08-01T02:28:00.009964",
     "exception": false,
     "start_time": "2025-08-01T02:27:59.999974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_subreddits():\n",
    "    global top_overall_subreddits\n",
    "    # --- NEW FILTERING LOGIC FOR SUBREDDITS ---\n",
    "    min_comments_for_subreddit_analysis = 10  # Minimum comments required for a subreddit to be considered\n",
    "    print(f\"\\nFiltering subreddits: requiring at least {min_comments_for_subreddit_analysis} comments and mean violation between 0% and 100%.\")\n",
    "    # Combine into a temporary DataFrame for easy filtering\n",
    "    subreddit_stats = pd.DataFrame(\n",
    "        {\n",
    "            'mean_violation': overall_subreddit_mean_violations, 'comment_count': num_comments_per_subreddit\n",
    "        }\n",
    "    )\n",
    "    # Apply the filtering conditions\n",
    "    filtered_subreddits = subreddit_stats[(subreddit_stats['comment_count'] >= min_comments_for_subreddit_analysis) & (subreddit_stats['mean_violation'] > 0) &  # Exclude 0% violation\n",
    "                                          (subreddit_stats['mean_violation'] < 1)  # Exclude 100% violation\n",
    "                                          ]\n",
    "    # Get the subreddits from this filtered list, sorted by their mean violation\n",
    "    return filtered_subreddits.sort_values('mean_violation', ascending=False).head(32).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0d62480",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T02:28:00.017905Z",
     "iopub.status.busy": "2025-08-01T02:28:00.017494Z",
     "iopub.status.idle": "2025-08-01T02:28:00.023307Z",
     "shell.execute_reply": "2025-08-01T02:28:00.022421Z"
    },
    "papermill": {
     "duration": 0.011051,
     "end_time": "2025-08-01T02:28:00.024532",
     "exception": false,
     "start_time": "2025-08-01T02:28:00.013481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def violations_filtered_subreddits(top_overall_subreddits):\n",
    "    global subreddit, current_subreddit_mean, current_subreddit_count\n",
    "    # --- Step 5: Identify Top Violators per Subreddit (using the filtered list) ---\n",
    "    num_subreddits_to_display = 16  # This will now be from the filtered list\n",
    "    num_keywords_per_subreddit = 16\n",
    "    print(f\"\\n--- Top {num_keywords_per_subreddit} Violation-Prone Keywords per Subreddit (for top {num_subreddits_to_display} *filtered* subreddits) ---\")\n",
    "    if not top_overall_subreddits:\n",
    "        print(\"No subreddits met the filtering criteria to display.\")\n",
    "    else:\n",
    "        for subreddit in top_overall_subreddits:\n",
    "            # Get the overall mean violation and comment count for the current subreddit\n",
    "            current_subreddit_mean = overall_subreddit_mean_violations.loc[subreddit]\n",
    "            current_subreddit_count = num_comments_per_subreddit.loc[subreddit]\n",
    "\n",
    "            print(f\"\\nSubreddit: r/{subreddit} (Comments: {current_subreddit_count}, Overall Mean Violation: {current_subreddit_mean:.4f})\")\n",
    "\n",
    "            # Filter for the current subreddit and sort by mean_violation\n",
    "            subreddit_top_keywords = significant_subreddit_keywords[significant_subreddit_keywords['subreddit'] == subreddit].sort_values('mean_violation', ascending=False).head(\n",
    "                num_keywords_per_subreddit\n",
    "                )\n",
    "\n",
    "            if not subreddit_top_keywords.empty:\n",
    "                print(subreddit_top_keywords[['keywords', 'count', 'mean_violation']].to_string(index=False))\n",
    "            else:\n",
    "                print(\"  No significant keywords found for this subreddit (after keyword occurrence filter).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc203616",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T02:28:00.032891Z",
     "iopub.status.busy": "2025-08-01T02:28:00.032685Z",
     "iopub.status.idle": "2025-08-01T02:28:00.037810Z",
     "shell.execute_reply": "2025-08-01T02:28:00.037245Z"
    },
    "papermill": {
     "duration": 0.010221,
     "end_time": "2025-08-01T02:28:00.038902",
     "exception": false,
     "start_time": "2025-08-01T02:28:00.028681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def violations_urls():\n",
    "    global url_pattern_check, significant_subreddit_urls\n",
    "    # Define the URL pattern again to filter keywords\n",
    "    url_pattern_check = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    # Assuming subreddit_keyword_df and url_pattern_check are already defined\n",
    "    # Filter the exploded DataFrame to only include URLs as keywords\n",
    "    # (This is the same url_keyword_df from Part 1)\n",
    "    url_keyword_df = subreddit_keyword_df[subreddit_keyword_df['keywords'].apply(lambda x: isinstance(x, str) and bool(url_pattern_check.match(x)))].copy()\n",
    "    # Group by subreddit and URL, then aggregate\n",
    "    print(\"\\nCalculating mean violation for each URL per subreddit...\")\n",
    "    subreddit_url_scores = url_keyword_df.groupby(['subreddit', 'keywords'])['rule_violation'].agg(['count', 'mean']).reset_index()\n",
    "    subreddit_url_scores.rename(columns={'mean': 'mean_violation', 'keywords': 'url'}, inplace=True)\n",
    "    # Filter for significance (e.g., URL appearing at least 3 times within a subreddit)\n",
    "    min_url_occurrences_per_subreddit = 3\n",
    "    significant_subreddit_urls = subreddit_url_scores[subreddit_url_scores['count'] >= min_url_occurrences_per_subreddit]\n",
    "    print(f\"Found {significant_subreddit_urls.shape[0]} significant (subreddit, URL) pairs (>= {min_url_occurrences_per_subreddit} occurrences).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0298ee2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T02:28:00.046714Z",
     "iopub.status.busy": "2025-08-01T02:28:00.046498Z",
     "iopub.status.idle": "2025-08-01T02:28:00.051822Z",
     "shell.execute_reply": "2025-08-01T02:28:00.051235Z"
    },
    "papermill": {
     "duration": 0.010611,
     "end_time": "2025-08-01T02:28:00.053044",
     "exception": false,
     "start_time": "2025-08-01T02:28:00.042433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def urls_subredditts():\n",
    "    global subreddit, current_subreddit_mean, current_subreddit_count\n",
    "    # --- Display Top URLs per Subreddit ---\n",
    "    # Reuse the filtered_subreddits and top_overall_subreddits from previous analysis\n",
    "    # (assuming they are still in your notebook's environment)\n",
    "    num_subreddits_to_display_urls = 5\n",
    "    num_urls_per_subreddit = 10\n",
    "    print(f\"\\n--- Top {num_urls_per_subreddit} Violation-Prone URLs per Subreddit (for top {num_subreddits_to_display_urls} filtered subreddits) ---\")\n",
    "    if not top_overall_subreddits:  # This list comes from the previous subreddit filtering\n",
    "        print(\"No subreddits met the filtering criteria to display URLs.\")\n",
    "    else:\n",
    "        for subreddit in top_overall_subreddits:\n",
    "            # Get the overall mean violation and comment count for the current subreddit\n",
    "            current_subreddit_mean = overall_subreddit_mean_violations.loc[subreddit]\n",
    "            current_subreddit_count = num_comments_per_subreddit.loc[subreddit]\n",
    "\n",
    "            print(f\"\\nSubreddit: r/{subreddit} (Comments: {current_subreddit_count}, Overall Mean Violation: {current_subreddit_mean:.4f})\")\n",
    "\n",
    "            # Filter for the current subreddit and sort by mean_violation\n",
    "            subreddit_top_urls = significant_subreddit_urls[significant_subreddit_urls['subreddit'] == subreddit].sort_values('mean_violation', ascending=False).head(num_urls_per_subreddit)\n",
    "\n",
    "            if not subreddit_top_urls.empty:\n",
    "                print(subreddit_top_urls[['url', 'count', 'mean_violation']].to_string(index=False))\n",
    "            else:\n",
    "                print(\"  No significant URLs found for this subreddit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7397c492",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T02:28:00.061329Z",
     "iopub.status.busy": "2025-08-01T02:28:00.060637Z",
     "iopub.status.idle": "2025-08-01T02:28:00.066822Z",
     "shell.execute_reply": "2025-08-01T02:28:00.066241Z"
    },
    "papermill": {
     "duration": 0.011308,
     "end_time": "2025-08-01T02:28:00.067929",
     "exception": false,
     "start_time": "2025-08-01T02:28:00.056621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def subreddit_body_violation():\n",
    "    global subreddit_keyword_df, significant_subreddit_keywords, overall_subreddit_mean_violations, num_comments_per_subreddit, top_overall_subreddits\n",
    "    # Ensure full comment text is displayed for later inspection\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    # --- Step 1: Explode the DataFrame by keywords (from previous code) ---\n",
    "    print(\"Exploding DataFrame by keywords...\")\n",
    "    subreddit_keyword_df = df_train.explode('keywords')\n",
    "    subreddit_keyword_df.dropna(subset=['keywords'], inplace=True)\n",
    "    print(f\"Shape after exploding: {subreddit_keyword_df.shape}\")\n",
    "    # --- Step 2 & 3: Group by subreddit and keyword, then aggregate (from previous code) ---\n",
    "    print(\"Grouping by subreddit and keyword to calculate scores...\")\n",
    "    subreddit_keyword_scores = subreddit_keyword_df.groupby(['subreddit', 'keywords'])['rule_violation'].agg(['count', 'mean']).reset_index()\n",
    "    subreddit_keyword_scores.rename(columns={'mean': 'mean_violation'}, inplace=True)\n",
    "    print(\"Calculation complete.\")\n",
    "    # --- Step 4: Filter for significance (from previous code) ---\n",
    "    min_occurrences_per_subreddit = 10\n",
    "    significant_subreddit_keywords = subreddit_keyword_scores[subreddit_keyword_scores['count'] >= min_occurrences_per_subreddit]\n",
    "    print(f\"Total unique (subreddit, keyword) pairs: {subreddit_keyword_scores.shape[0]}\")\n",
    "    print(f\"Significant (subreddit, keyword) pairs (>= {min_occurrences_per_subreddit} occurrences): {significant_subreddit_keywords.shape[0]}\")\n",
    "    # --- Calculate overall mean violation per subreddit AND number of comments per subreddit ---\n",
    "    overall_subreddit_mean_violations = df_train.groupby('subreddit')['rule_violation'].mean()\n",
    "    num_comments_per_subreddit = df_train.groupby('subreddit').size()\n",
    "    top_overall_subreddits = filter_subreddits()\n",
    "    violations_filtered_subreddits(top_overall_subreddits)\n",
    "    violations_urls()\n",
    "    urls_subredditts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5646bf38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T02:28:00.075823Z",
     "iopub.status.busy": "2025-08-01T02:28:00.075608Z",
     "iopub.status.idle": "2025-08-01T02:28:00.081895Z",
     "shell.execute_reply": "2025-08-01T02:28:00.081067Z"
    },
    "papermill": {
     "duration": 0.011891,
     "end_time": "2025-08-01T02:28:00.083272",
     "exception": false,
     "start_time": "2025-08-01T02:28:00.071381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def subreddit_body_violation():\n",
    "    global subreddit_keyword_df, significant_subreddit_keywords, overall_subreddit_mean_violations, num_comments_per_subreddit, top_overall_subreddits\n",
    "    # Ensure full comment text is displayed for later inspection\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    # --- Step 1: Explode the DataFrame by keywords (from previous code) ---\n",
    "    print(\"Exploding DataFrame by keywords...\")\n",
    "    subreddit_keyword_df = df_train.explode('keywords')\n",
    "    subreddit_keyword_df.dropna(subset=['keywords'], inplace=True)\n",
    "    print(f\"Shape after exploding: {subreddit_keyword_df.shape}\")\n",
    "    # --- Step 2 & 3: Group by subreddit and keyword, then aggregate (from previous code) ---\n",
    "    print(\"Grouping by subreddit and keyword to calculate scores...\")\n",
    "    subreddit_keyword_scores = subreddit_keyword_df.groupby(['subreddit', 'keywords'])['rule_violation'].agg(['count', 'mean']).reset_index()\n",
    "    subreddit_keyword_scores.rename(columns={'mean': 'mean_violation'}, inplace=True)\n",
    "    print(\"Calculation complete.\")\n",
    "    # --- Step 4: Filter for significance (from previous code) ---\n",
    "    min_occurrences_per_subreddit = 10\n",
    "    significant_subreddit_keywords = subreddit_keyword_scores[subreddit_keyword_scores['count'] >= min_occurrences_per_subreddit]\n",
    "    print(f\"Total unique (subreddit, keyword) pairs: {subreddit_keyword_scores.shape[0]}\")\n",
    "    print(f\"Significant (subreddit, keyword) pairs (>= {min_occurrences_per_subreddit} occurrences): {significant_subreddit_keywords.shape[0]}\")\n",
    "    # --- Calculate overall mean violation per subreddit AND number of comments per subreddit ---\n",
    "    overall_subreddit_mean_violations = df_train.groupby('subreddit')['rule_violation'].mean()\n",
    "    num_comments_per_subreddit = df_train.groupby('subreddit').size()\n",
    "    top_overall_subreddits = filter_subreddits()\n",
    "    violations_filtered_subreddits(top_overall_subreddits)\n",
    "    violations_urls()\n",
    "    urls_subredditts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1543bdbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T02:28:00.091590Z",
     "iopub.status.busy": "2025-08-01T02:28:00.090862Z",
     "iopub.status.idle": "2025-08-01T02:28:00.096812Z",
     "shell.execute_reply": "2025-08-01T02:28:00.096120Z"
    },
    "papermill": {
     "duration": 0.011179,
     "end_time": "2025-08-01T02:28:00.097853",
     "exception": false,
     "start_time": "2025-08-01T02:28:00.086674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rule_subredditt():\n",
    "    # Assuming df_full_train is already loaded\n",
    "    # df_full_train = pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/train.csv\")\n",
    "    print(\"--- Analyzing Rule-Subreddit Relationships ---\")\n",
    "    # 1. Number of unique rules per subreddit\n",
    "    print(\"\\n--- Number of Unique Rules per Subreddit ---\")\n",
    "    rules_per_subreddit = df_train.groupby('subreddit')['rule'].nunique().sort_values(ascending=False)\n",
    "    print(\"Top 10 subreddits by number of unique rules:\")\n",
    "    print(rules_per_subreddit.head(10))\n",
    "    print(\"\\nBottom 10 subreddits by number of unique rules:\")\n",
    "    print(rules_per_subreddit.tail(10))\n",
    "    print(f\"\\nTotal unique subreddits: {len(rules_per_subreddit)}\")\n",
    "    print(f\"Subreddits with only one rule: {len(rules_per_subreddit[rules_per_subreddit == 1])}\")\n",
    "    # 2. Number of unique subreddits per rule\n",
    "    print(\"\\n--- Number of Unique Subreddits per Rule ---\")\n",
    "    subreddits_per_rule = df_train.groupby('rule')['subreddit'].nunique().sort_values(ascending=False)\n",
    "    print(\"Top 10 rules by number of unique subreddits:\")\n",
    "    print(subreddits_per_rule.head(10))\n",
    "    print(\"\\nBottom 10 rules by number of unique subreddits:\")\n",
    "    print(subreddits_per_rule.tail(10))\n",
    "    print(f\"\\nTotal unique rules: {len(subreddits_per_rule)}\")\n",
    "    print(f\"Rules appearing in only one subreddit: {len(subreddits_per_rule[subreddits_per_rule == 1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8197c2e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T02:28:00.105870Z",
     "iopub.status.busy": "2025-08-01T02:28:00.105656Z",
     "iopub.status.idle": "2025-08-01T02:28:00.113727Z",
     "shell.execute_reply": "2025-08-01T02:28:00.113082Z"
    },
    "papermill": {
     "duration": 0.013418,
     "end_time": "2025-08-01T02:28:00.114890",
     "exception": false,
     "start_time": "2025-08-01T02:28:00.101472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rule_violations():\n",
    "    # Ensure full comment text is displayed for later inspection\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    # Assuming df_full_train is loaded and 'keywords' column is generated by clean_and_tokenize_with_urls\n",
    "    # (using df_full_train['body'].apply(clean_and_tokenize_with_urls))\n",
    "    # --- Step 1: Explode the DataFrame by keywords ---\n",
    "    # This creates a row for each keyword, duplicating the comment's rule and violation score.\n",
    "    print(\"Exploding DataFrame by keywords...\")\n",
    "    rule_keyword_df = df_train.explode('keywords')\n",
    "    rule_keyword_df.dropna(subset=['keywords'], inplace=True)\n",
    "    print(f\"Shape after exploding: {rule_keyword_df.shape}\")\n",
    "    # --- Step 2 & 3: Group by rule and keyword, then aggregate ---\n",
    "    print(\"Grouping by rule and keyword to calculate scores...\")\n",
    "    rule_keyword_scores = rule_keyword_df.groupby(['rule', 'keywords'])['rule_violation'].agg(['count', 'mean']).reset_index()\n",
    "    rule_keyword_scores.rename(columns={'mean': 'mean_violation'}, inplace=True)\n",
    "    print(\"Calculation complete.\")\n",
    "    # --- Step 4: Filter for significance ---\n",
    "    # A word needs to appear at least 'min_occurrences_per_rule' times within a rule\n",
    "    # to be considered for analysis. Adjust this threshold as needed.\n",
    "    min_occurrences_per_rule = 16  # Similar to subreddit, adjust based on data\n",
    "    significant_rule_keywords = rule_keyword_scores[rule_keyword_scores['count'] >= min_occurrences_per_rule]\n",
    "    print(f\"Total unique (rule, keyword) pairs: {rule_keyword_scores.shape[0]}\")\n",
    "    print(f\"Significant (rule, keyword) pairs (>= {min_occurrences_per_rule} occurrences): {significant_rule_keywords.shape[0]}\")\n",
    "    # --- NEW: Calculate overall mean violation per rule AND number of comments per rule ---\n",
    "    overall_rule_mean_violations = df_train.groupby('rule')['rule_violation'].mean()\n",
    "    num_comments_per_rule = df_train.groupby('rule').size()\n",
    "    # --- Step 5: Identify Top Violators per Rule ---\n",
    "    num_rules_to_display = 2\n",
    "    num_keywords_per_rule = 16\n",
    "    # Get the rules with the highest overall average violation scores (for display order)\n",
    "    # We'll apply similar filtering for rules as we did for subreddits to avoid skewed results\n",
    "    min_comments_for_rule_analysis = 4  # Minimum comments required for a rule to be considered\n",
    "    rule_stats = pd.DataFrame(\n",
    "        {\n",
    "            'mean_violation': overall_rule_mean_violations, 'comment_count': num_comments_per_rule\n",
    "        }\n",
    "    )\n",
    "    filtered_rules = rule_stats[(rule_stats['comment_count'] >= min_comments_for_rule_analysis) & (rule_stats['mean_violation'] > 0) & (rule_stats['mean_violation'] < 1)]\n",
    "    top_overall_rules = filtered_rules.sort_values('mean_violation', ascending=False).head(num_rules_to_display).index.tolist()\n",
    "    print(f\"\\n--- Top {num_keywords_per_rule} Violation-Prone Keywords per Rule (for top {num_rules_to_display} *filtered* rules) ---\")\n",
    "    if not top_overall_rules:\n",
    "        print(\"No rules met the filtering criteria to display.\")\n",
    "    else:\n",
    "        for rule in top_overall_rules:\n",
    "            # Get the overall mean violation and comment count for the current rule\n",
    "            current_rule_mean = overall_rule_mean_violations.loc[rule]\n",
    "            current_rule_count = num_comments_per_rule.loc[rule]\n",
    "\n",
    "            print(f\"\\nRule: {rule} (Comments: {current_rule_count}, Overall Mean Violation: {current_rule_mean:.4f})\")\n",
    "\n",
    "            # Filter for the current rule and sort by mean_violation\n",
    "            rule_top_keywords = significant_rule_keywords[significant_rule_keywords['rule'] == rule].sort_values('mean_violation', ascending=False).head(num_keywords_per_rule)\n",
    "\n",
    "            if not rule_top_keywords.empty:\n",
    "                print(rule_top_keywords[['keywords', 'count', 'mean_violation']].to_string(index=False))\n",
    "            else:\n",
    "                print(\"  No significant keywords found for this rule (after keyword occurrence filter).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac64930a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T02:28:00.122579Z",
     "iopub.status.busy": "2025-08-01T02:28:00.122355Z",
     "iopub.status.idle": "2025-08-01T02:28:00.126002Z",
     "shell.execute_reply": "2025-08-01T02:28:00.125494Z"
    },
    "papermill": {
     "duration": 0.008621,
     "end_time": "2025-08-01T02:28:00.127014",
     "exception": false,
     "start_time": "2025-08-01T02:28:00.118393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value) # For all GPUs\n",
    "    # For deterministic behavior (can slow down training)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06b8148b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T02:28:00.135080Z",
     "iopub.status.busy": "2025-08-01T02:28:00.134839Z",
     "iopub.status.idle": "2025-08-01T02:28:00.143323Z",
     "shell.execute_reply": "2025-08-01T02:28:00.142699Z"
    },
    "papermill": {
     "duration": 0.013853,
     "end_time": "2025-08-01T02:28:00.144350",
     "exception": false,
     "start_time": "2025-08-01T02:28:00.130497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_datasets():\n",
    "    # --- Tokenizer ---\n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_PATH,  use_fast=False)\n",
    "\n",
    "    # --- Dataset Class ---\n",
    "    class JigsawDataset(Dataset):\n",
    "        def __init__(self, dataframe, tokenizer, max_len, is_test=False):\n",
    "            self.dataframe = dataframe\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_len = max_len\n",
    "            self.is_test = is_test\n",
    "\n",
    "            # Create the enriched rule string for both train and test\n",
    "            # Use .fillna('') to handle potential NaN values in example columns gracefully\n",
    "            # This is the core of the \"enriched rule\" strategy\n",
    "            self.dataframe['enriched_rule'] = (\n",
    "                \"RULE: \" + self.dataframe['rule'] + \" POS_EX: \" + self.dataframe['positive_example_1'].fillna('') + \" \" + self.dataframe['positive_example_2'].fillna('') + \" NEG_EX: \" +\n",
    "                self.dataframe['negative_example_1'].fillna('') + \" \" + self.dataframe['negative_example_2'].fillna(''))\n",
    "\n",
    "            # Combine 'body' and 'enriched_rule' for input to the model\n",
    "            # The tokenizer.sep_token acts as a separator that the model understands\n",
    "            # This creates a single input string for the transformer\n",
    "            self.texts = self.dataframe.apply(lambda x: x['body'] + tokenizer.sep_token + x['enriched_rule'], axis=1)\n",
    "\n",
    "            if not is_test:\n",
    "                self.labels = self.dataframe['rule_violation'].values  # No else needed for test, as labels are not used for test\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dataframe)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            text = str(self.texts.iloc[idx])\n",
    "\n",
    "            # Tokenize the text\n",
    "            encoding = self.tokenizer.encode_plus(\n",
    "                text, add_special_tokens=True, max_length=self.max_len, return_token_type_ids=False,  # RoBERTa and XLM-R models typically don't use token_type_ids\n",
    "                padding='max_length', truncation=True, return_attention_mask=True, return_tensors='pt', )\n",
    "\n",
    "            inputs = {\n",
    "                'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten()\n",
    "            }\n",
    "\n",
    "            if not self.is_test:\n",
    "                # Ensure labels are float32 and have a shape of [1] for BCEWithLogitsLoss\n",
    "                inputs['labels'] = torch.tensor([self.labels[idx]], dtype=torch.float32)\n",
    "\n",
    "            return inputs\n",
    "\n",
    "    # --- Create Datasets and DataLoaders ---\n",
    "    MAX_LEN = 256  # Max sequence length for DistilBERT. Adjust based on your text length.\n",
    "    BATCH_SIZE = 16  # Smaller batch size for memory constraints\n",
    "    # Split training data for local validation (good practice)\n",
    "    train_df, val_df = train_test_split(df_train, test_size=0.1, random_state=42)\n",
    "    train_dataset = JigsawDataset(train_df, tokenizer, MAX_LEN)\n",
    "    val_dataset = JigsawDataset(val_df, tokenizer, MAX_LEN)\n",
    "    test_dataset = JigsawDataset(df_test, tokenizer, MAX_LEN, is_test=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    print(\"Datasets and DataLoaders created.\")\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ca5c8f5",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-08-01T02:28:00.153158Z",
     "iopub.status.busy": "2025-08-01T02:28:00.152904Z",
     "iopub.status.idle": "2025-08-01T02:28:00.623924Z",
     "shell.execute_reply": "2025-08-01T02:28:00.623075Z"
    },
    "papermill": {
     "duration": 0.478006,
     "end_time": "2025-08-01T02:28:00.626049",
     "exception": false,
     "start_time": "2025-08-01T02:28:00.148043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying URL-aware cleaning function...\n",
      "Cleaning complete.\n",
      "Shape before exploding: (2029, 11)\n",
      "Shape after exploding: (29717, 11)\n",
      "\n",
      "Calculating word scores... (This may take a minute)\n",
      "Calculation complete.\n",
      "Total unique words: 7601\n",
      "Significant words (>= 64 occurrences): 37\n",
      "Searching for high-violation keywords like: ['legal', 'law', 'illegal', 'com', 'll']\n",
      "Searching for low-violation keywords like: ['sd', 'english', 'hd', 'stream', 'mobile']\n",
      "Exploding DataFrame by keywords...\n",
      "Shape after exploding: (29717, 11)\n",
      "Grouping by subreddit and keyword to calculate scores...\n",
      "Calculation complete.\n",
      "Total unique (subreddit, keyword) pairs: 17935\n",
      "Significant (subreddit, keyword) pairs (>= 10 occurrences): 204\n",
      "\n",
      "Filtering subreddits: requiring at least 10 comments and mean violation between 0% and 100%.\n",
      "\n",
      "--- Top 16 Violation-Prone Keywords per Subreddit (for top 16 *filtered* subreddits) ---\n",
      "\n",
      "Subreddit: r/churning (Comments: 21, Overall Mean Violation: 0.9048)\n",
      "   keywords  count  mean_violation\n",
      "        000     16        1.000000\n",
      "      chase     10        1.000000\n",
      "     points     10        1.000000\n",
      "placeholder     21        0.904762\n",
      "        url     21        0.904762\n",
      "\n",
      "Subreddit: r/sex (Comments: 42, Overall Mean Violation: 0.8095)\n",
      "   keywords  count  mean_violation\n",
      "placeholder     23        0.826087\n",
      "        url     23        0.826087\n",
      "        sex     17        0.823529\n",
      "\n",
      "Subreddit: r/legaladvice (Comments: 213, Overall Mean Violation: 0.7887)\n",
      "keywords  count  mean_violation\n",
      "attorney     14        1.000000\n",
      "  lawyer     26        1.000000\n",
      "     sue     20        1.000000\n",
      " illegal     12        1.000000\n",
      "   legal     25        0.960000\n",
      "      ll     17        0.941176\n",
      "  police     29        0.931034\n",
      "  advice     14        0.928571\n",
      "   child     13        0.923077\n",
      "   state     13        0.923077\n",
      "   court     13        0.923077\n",
      "   think     13        0.923077\n",
      "     way     12        0.916667\n",
      "    just     34        0.911765\n",
      "    help     10        0.900000\n",
      "     did     17        0.882353\n",
      "\n",
      "Subreddit: r/GlobalOffensive (Comments: 18, Overall Mean Violation: 0.7778)\n",
      "   keywords  count  mean_violation\n",
      "       free     12            1.00\n",
      "      promo     10            1.00\n",
      "placeholder     12            0.75\n",
      "        url     12            0.75\n",
      "\n",
      "Subreddit: r/hearthstone (Comments: 22, Overall Mean Violation: 0.7727)\n",
      "   keywords  count  mean_violation\n",
      "    tyrande     14        1.000000\n",
      "         pm     11        0.909091\n",
      "       code     18        0.888889\n",
      "placeholder     21        0.809524\n",
      "        url     21        0.809524\n",
      "\n",
      "Subreddit: r/nottheonion (Comments: 11, Overall Mean Violation: 0.7273)\n",
      "  No significant keywords found for this subreddit (after keyword occurrence filter).\n",
      "\n",
      "Subreddit: r/canada (Comments: 11, Overall Mean Violation: 0.7273)\n",
      "  No significant keywords found for this subreddit (after keyword occurrence filter).\n",
      "\n",
      "Subreddit: r/aww (Comments: 18, Overall Mean Violation: 0.7222)\n",
      "   keywords  count  mean_violation\n",
      "placeholder     15        0.733333\n",
      "        url     15        0.733333\n",
      "\n",
      "Subreddit: r/personalfinance (Comments: 125, Overall Mean Violation: 0.6960)\n",
      "   keywords  count  mean_violation\n",
      "       hack     10        1.000000\n",
      "      taxes     12        0.916667\n",
      "        tax     10        0.900000\n",
      "        irs     10        0.900000\n",
      "       file     10        0.900000\n",
      "        pay     16        0.875000\n",
      "        job     10        0.800000\n",
      "        got     10        0.800000\n",
      "        don     24        0.791667\n",
      "       just     23        0.782609\n",
      "       tell     10        0.700000\n",
      "      money     23        0.608696\n",
      "       look     10        0.600000\n",
      "        url     19        0.526316\n",
      "placeholder     19        0.526316\n",
      "       like     12        0.500000\n",
      "\n",
      "Subreddit: r/explainlikeimfive (Comments: 14, Overall Mean Violation: 0.6429)\n",
      "  No significant keywords found for this subreddit (after keyword occurrence filter).\n",
      "\n",
      "Subreddit: r/books (Comments: 11, Overall Mean Violation: 0.6364)\n",
      "  No significant keywords found for this subreddit (after keyword occurrence filter).\n",
      "\n",
      "Subreddit: r/Showerthoughts (Comments: 29, Overall Mean Violation: 0.6207)\n",
      "   keywords  count  mean_violation\n",
      "placeholder     24           0.625\n",
      "        url     24           0.625\n",
      "\n",
      "Subreddit: r/NSFW_GIF (Comments: 13, Overall Mean Violation: 0.6154)\n",
      "   keywords  count  mean_violation\n",
      "placeholder     22        0.681818\n",
      "        url     22        0.681818\n",
      "\n",
      "Subreddit: r/relationships (Comments: 106, Overall Mean Violation: 0.6132)\n",
      "keywords  count  mean_violation\n",
      "  lawyer     10        1.000000\n",
      "     don     24        0.875000\n",
      " divorce     11        0.818182\n",
      "    like     11        0.727273\n",
      "   going     11        0.727273\n",
      "     man     11        0.727273\n",
      "    want     11        0.727273\n",
      "  people     12        0.666667\n",
      "    know     12        0.666667\n",
      "     way     10        0.600000\n",
      "    tell     17        0.588235\n",
      "      op     12        0.583333\n",
      "    make     14        0.571429\n",
      "     pay     14        0.571429\n",
      "    rape     17        0.529412\n",
      "    just     16        0.437500\n",
      "\n",
      "Subreddit: r/politics (Comments: 49, Overall Mean Violation: 0.5918)\n",
      "keywords  count  mean_violation\n",
      " illegal     12        0.416667\n",
      "\n",
      "Subreddit: r/pokemon (Comments: 22, Overall Mean Violation: 0.5909)\n",
      "   keywords  count  mean_violation\n",
      "placeholder     18        0.611111\n",
      "        url     18        0.611111\n",
      "\n",
      "Subreddit: r/hillaryclinton (Comments: 22, Overall Mean Violation: 0.5909)\n",
      "  No significant keywords found for this subreddit (after keyword occurrence filter).\n",
      "\n",
      "Subreddit: r/news (Comments: 65, Overall Mean Violation: 0.5846)\n",
      "   keywords  count  mean_violation\n",
      "placeholder     10             0.4\n",
      "        url     10             0.4\n",
      "       just     10             0.3\n",
      "\n",
      "Subreddit: r/pics (Comments: 30, Overall Mean Violation: 0.5667)\n",
      "   keywords  count  mean_violation\n",
      "placeholder     26        0.538462\n",
      "        url     26        0.538462\n",
      "\n",
      "Subreddit: r/The_Donald (Comments: 94, Overall Mean Violation: 0.5532)\n",
      "   keywords  count  mean_violation\n",
      "     people     14        0.642857\n",
      "       just     17        0.470588\n",
      "      trump     16        0.437500\n",
      "placeholder     18        0.388889\n",
      "        url     18        0.388889\n",
      "\n",
      "Subreddit: r/gifs (Comments: 21, Overall Mean Violation: 0.5238)\n",
      "   keywords  count  mean_violation\n",
      "placeholder     16          0.4375\n",
      "        url     16          0.4375\n",
      "\n",
      "Subreddit: r/nosleep (Comments: 12, Overall Mean Violation: 0.5000)\n",
      "  No significant keywords found for this subreddit (after keyword occurrence filter).\n",
      "\n",
      "Subreddit: r/dataisbeautiful (Comments: 10, Overall Mean Violation: 0.5000)\n",
      "  No significant keywords found for this subreddit (after keyword occurrence filter).\n",
      "\n",
      "Subreddit: r/pcmasterrace (Comments: 16, Overall Mean Violation: 0.5000)\n",
      "  No significant keywords found for this subreddit (after keyword occurrence filter).\n",
      "\n",
      "Subreddit: r/LifeProTips (Comments: 14, Overall Mean Violation: 0.5000)\n",
      "  No significant keywords found for this subreddit (after keyword occurrence filter).\n",
      "\n",
      "Subreddit: r/OldSchoolCool (Comments: 10, Overall Mean Violation: 0.5000)\n",
      "   keywords  count  mean_violation\n",
      "    writing     11        1.000000\n",
      "placeholder     11        0.636364\n",
      "        url     11        0.636364\n",
      "\n",
      "Subreddit: r/depression (Comments: 18, Overall Mean Violation: 0.5000)\n",
      "  No significant keywords found for this subreddit (after keyword occurrence filter).\n",
      "\n",
      "Subreddit: r/AskReddit (Comments: 152, Overall Mean Violation: 0.4803)\n",
      "         keywords  count  mean_violation\n",
      "             fast     25        0.800000\n",
      "           reddit     24        0.791667\n",
      "           glitch     24        0.791667\n",
      "          patched     24        0.791667\n",
      "            karma     24        0.791667\n",
      "www.freekarma.com     24        0.791667\n",
      "              url    113        0.734513\n",
      "             free     33        0.727273\n",
      "      placeholder     98        0.693878\n",
      "            check     27        0.148148\n",
      "      interesting     24        0.000000\n",
      "   dailyetymology     23        0.000000\n",
      "        etymology     22        0.000000\n",
      "             cool     19        0.000000\n",
      "        instagram     23        0.000000\n",
      "             page     23        0.000000\n",
      "\n",
      "Subreddit: r/gaming (Comments: 15, Overall Mean Violation: 0.4667)\n",
      "   keywords  count  mean_violation\n",
      "placeholder     11        0.363636\n",
      "        url     11        0.363636\n",
      "\n",
      "Subreddit: r/TwoXChromosomes (Comments: 87, Overall Mean Violation: 0.4598)\n",
      "keywords  count  mean_violation\n",
      "     sex     22        0.636364\n",
      "   child     16        0.562500\n",
      "     say     11        0.545455\n",
      "     don     15        0.466667\n",
      "    rape     27        0.407407\n",
      "    like     13        0.384615\n",
      "    just     10        0.300000\n",
      "  sexual     10        0.200000\n",
      "\n",
      "Subreddit: r/DIY (Comments: 11, Overall Mean Violation: 0.4545)\n",
      "  No significant keywords found for this subreddit (after keyword occurrence filter).\n",
      "\n",
      "Subreddit: r/funny (Comments: 22, Overall Mean Violation: 0.4091)\n",
      "   keywords  count  mean_violation\n",
      "placeholder     17        0.294118\n",
      "        url     17        0.294118\n",
      "\n",
      "Calculating mean violation for each URL per subreddit...\n",
      "Found 24 significant (subreddit, URL) pairs (>= 3 occurrences).\n",
      "\n",
      "--- Top 10 Violation-Prone URLs per Subreddit (for top 5 filtered subreddits) ---\n",
      "\n",
      "Subreddit: r/churning (Comments: 21, Overall Mean Violation: 0.9048)\n",
      "  No significant URLs found for this subreddit.\n",
      "\n",
      "Subreddit: r/sex (Comments: 42, Overall Mean Violation: 0.8095)\n",
      "  No significant URLs found for this subreddit.\n",
      "\n",
      "Subreddit: r/legaladvice (Comments: 213, Overall Mean Violation: 0.7887)\n",
      "  No significant URLs found for this subreddit.\n",
      "\n",
      "Subreddit: r/GlobalOffensive (Comments: 18, Overall Mean Violation: 0.7778)\n",
      "                        url  count  mean_violation\n",
      "http://hellcase.com/f405290      4             1.0\n",
      "\n",
      "Subreddit: r/hearthstone (Comments: 22, Overall Mean Violation: 0.7727)\n",
      "                     url  count  mean_violation\n",
      "http://imgur.com/a/IMARW      4             1.0\n",
      "\n",
      "Subreddit: r/nottheonion (Comments: 11, Overall Mean Violation: 0.7273)\n",
      "  No significant URLs found for this subreddit.\n",
      "\n",
      "Subreddit: r/canada (Comments: 11, Overall Mean Violation: 0.7273)\n",
      "  No significant URLs found for this subreddit.\n",
      "\n",
      "Subreddit: r/aww (Comments: 18, Overall Mean Violation: 0.7222)\n",
      "                              url  count  mean_violation\n",
      "http://y2u.be/..https://bam.bz/OY      3             1.0\n",
      "\n",
      "Subreddit: r/personalfinance (Comments: 125, Overall Mean Violation: 0.6960)\n",
      "                       url  count  mean_violation\n",
      "https://senzu.io/investing      3             0.0\n",
      "\n",
      "Subreddit: r/explainlikeimfive (Comments: 14, Overall Mean Violation: 0.6429)\n",
      "  No significant URLs found for this subreddit.\n",
      "\n",
      "Subreddit: r/books (Comments: 11, Overall Mean Violation: 0.6364)\n",
      "  No significant URLs found for this subreddit.\n",
      "\n",
      "Subreddit: r/Showerthoughts (Comments: 29, Overall Mean Violation: 0.6207)\n",
      "                       url  count  mean_violation\n",
      "http://www.flickmaza.com/)      3             0.0\n",
      "\n",
      "Subreddit: r/NSFW_GIF (Comments: 13, Overall Mean Violation: 0.6154)\n",
      "  No significant URLs found for this subreddit.\n",
      "\n",
      "Subreddit: r/relationships (Comments: 106, Overall Mean Violation: 0.6132)\n",
      "  No significant URLs found for this subreddit.\n",
      "\n",
      "Subreddit: r/politics (Comments: 49, Overall Mean Violation: 0.5918)\n",
      "  No significant URLs found for this subreddit.\n",
      "\n",
      "Subreddit: r/pokemon (Comments: 22, Overall Mean Violation: 0.5909)\n",
      "  No significant URLs found for this subreddit.\n",
      "\n",
      "Subreddit: r/hillaryclinton (Comments: 22, Overall Mean Violation: 0.5909)\n",
      "  No significant URLs found for this subreddit.\n",
      "\n",
      "Subreddit: r/news (Comments: 65, Overall Mean Violation: 0.5846)\n",
      "  No significant URLs found for this subreddit.\n",
      "\n",
      "Subreddit: r/pics (Comments: 30, Overall Mean Violation: 0.5667)\n",
      "  No significant URLs found for this subreddit.\n",
      "\n",
      "Subreddit: r/The_Donald (Comments: 94, Overall Mean Violation: 0.5532)\n",
      "  No significant URLs found for this subreddit.\n",
      "\n",
      "Subreddit: r/gifs (Comments: 21, Overall Mean Violation: 0.5238)\n",
      "  No significant URLs found for this subreddit.\n",
      "\n",
      "Subreddit: r/nosleep (Comments: 12, Overall Mean Violation: 0.5000)\n",
      "  No significant URLs found for this subreddit.\n",
      "\n",
      "Subreddit: r/dataisbeautiful (Comments: 10, Overall Mean Violation: 0.5000)\n",
      "  No significant URLs found for this subreddit.\n",
      "\n",
      "Subreddit: r/pcmasterrace (Comments: 16, Overall Mean Violation: 0.5000)\n",
      "  No significant URLs found for this subreddit.\n",
      "\n",
      "Subreddit: r/LifeProTips (Comments: 14, Overall Mean Violation: 0.5000)\n",
      "  No significant URLs found for this subreddit.\n",
      "\n",
      "Subreddit: r/OldSchoolCool (Comments: 10, Overall Mean Violation: 0.5000)\n",
      "  No significant URLs found for this subreddit.\n",
      "\n",
      "Subreddit: r/depression (Comments: 18, Overall Mean Violation: 0.5000)\n",
      "  No significant URLs found for this subreddit.\n",
      "\n",
      "Subreddit: r/AskReddit (Comments: 152, Overall Mean Violation: 0.4803)\n",
      "              url  count  mean_violation\n",
      "www.freekarma.com     24        0.791667\n",
      "\n",
      "Subreddit: r/gaming (Comments: 15, Overall Mean Violation: 0.4667)\n",
      "  No significant URLs found for this subreddit.\n",
      "\n",
      "Subreddit: r/TwoXChromosomes (Comments: 87, Overall Mean Violation: 0.4598)\n",
      "                                                 url  count  mean_violation\n",
      "https://love-your-girl.myshopify.com/pages/giveaway1      3             1.0\n",
      "\n",
      "Subreddit: r/DIY (Comments: 11, Overall Mean Violation: 0.4545)\n",
      "  No significant URLs found for this subreddit.\n",
      "\n",
      "Subreddit: r/funny (Comments: 22, Overall Mean Violation: 0.4091)\n",
      "                           url  count  mean_violation\n",
      "https://telegram.me/PUREmusiic      3             0.0\n",
      "Exploding DataFrame by keywords...\n",
      "Shape after exploding: (29717, 11)\n",
      "Grouping by rule and keyword to calculate scores...\n",
      "Calculation complete.\n",
      "Total unique (rule, keyword) pairs: 8906\n",
      "Significant (rule, keyword) pairs (>= 16 occurrences): 267\n",
      "\n",
      "--- Top 16 Violation-Prone Keywords per Rule (for top 2 *filtered* rules) ---\n",
      "\n",
      "Rule: No legal advice: Do not offer or request legal advice. (Comments: 1017, Overall Mean Violation: 0.5831)\n",
      "keywords  count  mean_violation\n",
      "attorney     21        1.000000\n",
      "     sue     42        0.976190\n",
      "  lawyer     49        0.959184\n",
      " support     23        0.956522\n",
      "    file     19        0.947368\n",
      " legally     37        0.945946\n",
      "   claim     16        0.937500\n",
      "  police     63        0.936508\n",
      "  rights     22        0.909091\n",
      "  family     20        0.900000\n",
      "   court     29        0.896552\n",
      "    laws     28        0.892857\n",
      "   legal     93        0.892473\n",
      " charges     18        0.888889\n",
      "property     36        0.888889\n",
      "    cops     27        0.888889\n",
      "\n",
      "Rule: No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed. (Comments: 1012, Overall Mean Violation: 0.4328)\n",
      "         keywords  count  mean_violation\n",
      "              000     18        1.000000\n",
      "              sex     47        1.000000\n",
      "          selling     19        1.000000\n",
      "            girls     24        1.000000\n",
      "             girl     29        0.965517\n",
      "             earn     18        0.944444\n",
      "             code     34        0.911765\n",
      "          writing     17        0.882353\n",
      "            email     21        0.857143\n",
      "              buy     25        0.840000\n",
      "             just     50        0.820000\n",
      "           glitch     24        0.791667\n",
      "          patched     24        0.791667\n",
      "www.freekarma.com     24        0.791667\n",
      "            karma     24        0.791667\n",
      "            offer     17        0.764706\n"
     ]
    }
   ],
   "source": [
    "# Path to the training data inside your Kaggle Notebook\n",
    "trn_file = \"/kaggle/input/jigsaw-agile-community-rules/train.csv\"\n",
    "tst_file = \"/kaggle/input/jigsaw-agile-community-rules/test.csv\"\n",
    "\n",
    "# Load it into a pandas DataFrame\n",
    "df_train = pd.read_csv(trn_file)\n",
    "df_test  = pd.read_csv(tst_file)\n",
    "\n",
    "# prelim_explore()\n",
    "\n",
    "# Create a new feature for comment length\n",
    "df_train['comment_length'] = df_train['body'].str.len()\n",
    "\n",
    "# violations_subreddit()\n",
    "\n",
    "# --- Create a 'keywords' column ---\n",
    "print(\"Applying URL-aware cleaning function...\")\n",
    "df_train['keywords'] = df_train['body'].apply(clean_and_tokenize_with_urls)\n",
    "print(\"Cleaning complete.\")\n",
    "\n",
    "# pd.set_option('display.max_colwidth', 300)\n",
    "# print(df_train[df_train['body'].str.contains(r'[^a-zA-Z0-9\\s]', na=False)][['body', 'keywords']].head())\n",
    "significant_words = build_keywords()\n",
    "\n",
    "# --- Words with the HIGHEST violation scores ---\n",
    "top_violators = significant_words.sort_values('mean_violation', ascending=False)\n",
    "\n",
    "# print(\"\\n--- Top 20 Words Correlated with HIGH Violation Scores ---\")\n",
    "# print(top_violators.head(20))\n",
    "\n",
    "# --- Words with the LOWEST violation scores ---\n",
    "bottom_violators = significant_words.sort_values('mean_violation', ascending=True)\n",
    "\n",
    "# print(\"\\n--- Top 20 Words Correlated with LOW Violation Scores ---\")\n",
    "# print(bottom_violators.head(20))\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# --- 1. Get the lists of keywords ---\n",
    "num_keywords_to_search = 64\n",
    "top_keywords_list = top_violators.head(num_keywords_to_search)['keywords'].tolist()\n",
    "bottom_keywords_list = bottom_violators.head(num_keywords_to_search)['keywords'].tolist()\n",
    "\n",
    "# --- 2. Build the regex patterns ---\n",
    "top_regex = '|'.join([re.escape(word) for word in top_keywords_list])\n",
    "bottom_regex = '|'.join([re.escape(word) for word in bottom_keywords_list])\n",
    "\n",
    "print(f\"Searching for high-violation keywords like: {top_keywords_list[:5]}\")\n",
    "print(f\"Searching for low-violation keywords like: {bottom_keywords_list[:5]}\")\n",
    "\n",
    "# --- 3. Filter the DataFrame to find comments containing these words ---\n",
    "high_violation_examples = df_train[df_train['body'].str.contains(top_regex, case=False, na=False)]\n",
    "low_violation_examples = df_train[df_train['body'].str.contains(bottom_regex, case=False, na=False)]\n",
    "\n",
    "subreddit_body_violation()\n",
    "# rule_subredditt()\n",
    "rule_violations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a819c560",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T02:28:00.637368Z",
     "iopub.status.busy": "2025-08-01T02:28:00.637069Z",
     "iopub.status.idle": "2025-08-01T02:35:58.793975Z",
     "shell.execute_reply": "2025-08-01T02:35:58.792891Z"
    },
    "papermill": {
     "duration": 478.163989,
     "end_time": "2025-08-01T02:35:58.795525",
     "exception": false,
     "start_time": "2025-08-01T02:28:00.631536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA CUDA GPU: Tesla P100-PCIE-16GB\n",
      "Datasets and DataLoaders created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-01 02:28:11.265735: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754015291.475553      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754015291.532639      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 8 epoch(s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training: 100%|| 115/115 [00:54<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Train Loss: 0.6964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation: 100%|| 13/13 [00:01<00:00,  7.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Val Loss: 0.6935 - Val AUC: 0.5807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|| 115/115 [00:54<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Avg Train Loss: 0.6882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation: 100%|| 13/13 [00:01<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Avg Val Loss: 0.6882 - Val AUC: 0.5879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|| 115/115 [00:54<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Avg Train Loss: 0.6818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation: 100%|| 13/13 [00:01<00:00,  7.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Avg Val Loss: 0.6821 - Val AUC: 0.6315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training: 100%|| 115/115 [00:54<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Avg Train Loss: 0.6244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation: 100%|| 13/13 [00:01<00:00,  7.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Avg Val Loss: 0.5704 - Val AUC: 0.7418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training: 100%|| 115/115 [00:54<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Avg Train Loss: 0.5330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation: 100%|| 13/13 [00:01<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Avg Val Loss: 0.5158 - Val AUC: 0.8224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Training: 100%|| 115/115 [00:54<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Avg Train Loss: 0.4371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Validation: 100%|| 13/13 [00:01<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Avg Val Loss: 0.5163 - Val AUC: 0.8330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Training: 100%|| 115/115 [00:54<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Avg Train Loss: 0.3860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Validation: 100%|| 13/13 [00:01<00:00,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Avg Val Loss: 0.5076 - Val AUC: 0.8557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Training: 100%|| 115/115 [00:54<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Avg Train Loss: 0.3145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Validation: 100%|| 13/13 [00:01<00:00,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Avg Val Loss: 0.5044 - Val AUC: 0.8640\n",
      "Training complete.\n",
      "Making predictions on test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on Test Data: 100%|| 1/1 [00:00<00:00, 11.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions generated.\n",
      "Creating submission.csv file...\n",
      "submission.csv created successfully!\n",
      "   row_id  rule_violation\n",
      "0    2029        0.037383\n",
      "1    2030        0.916660\n",
      "2    2031        0.987551\n",
      "3    2032        0.988394\n",
      "4    2033        0.950249\n",
      "5    2034        0.009803\n",
      "6    2035        0.968696\n",
      "7    2036        0.019186\n",
      "8    2037        0.014613\n",
      "9    2038        0.977430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- The device ---\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using NVIDIA CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"NVIDIA CUDA GPU not available, using CPU.\")\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "MODEL_PATH = \"/kaggle/input/xlm-roberta-base-offline/xlm_roberta_base_offline\"\n",
    "\n",
    "train_dataset, val_dataset, test_dataset, train_loader, val_loader, test_loader = create_datasets()\n",
    "\n",
    "# --- Load Model ---\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=1)\n",
    "model.to(device) # Move model to device\n",
    "\n",
    "# --- Optimizer and Loss Function ---\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "# BCEWithLogitsLoss is good for binary classification when the model outputs logits (raw scores)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# --- Training Parameters ---\n",
    "N_EPOCHS = 8\n",
    "\n",
    "print(f\"Starting training for {N_EPOCHS} epoch(s)...\")\n",
    "for epoch in range(N_EPOCHS):\n",
    "    model.train() # Set model to training mode\n",
    "    train_loss = 0\n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\")):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device) # Labels will now be [batch_size, 1]\n",
    "\n",
    "        optimizer.zero_grad() # Clear gradients\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # *** FIX HERE: Keep logits as [batch_size, 1] for BCEWithLogitsLoss ***\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = loss_fn(logits, labels) # Both logits and labels are [batch_size, 1]\n",
    "        loss.backward() # Backpropagation\n",
    "        optimizer.step() # Update weights\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} - Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # --- Validation (Optional, but good for monitoring) ---\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    val_preds = []\n",
    "    val_true = []\n",
    "    val_loss = 0\n",
    "    with torch.no_grad(): # Disable gradient calculation for validation\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device) # Labels will be [batch_size, 1]\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # *** FIX HERE: Keep logits as [batch_size, 1] ***\n",
    "            logits = outputs.logits\n",
    "\n",
    "            loss = loss_fn(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # *** FIX HERE: Squeeze before sigmoid for prediction ***\n",
    "            preds = torch.sigmoid(logits.squeeze(-1)).cpu().numpy() # Convert logits to probabilities\n",
    "            val_preds.extend(preds)\n",
    "            val_true.extend(labels.squeeze(-1).cpu().numpy()) # Squeeze labels for AUC calculation\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_auc = roc_auc_score(val_true, val_preds)\n",
    "    print(f\"Epoch {epoch+1} - Avg Val Loss: {avg_val_loss:.4f} - Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- Make Predictions on Test Data ---\n",
    "print(\"Making predictions on test data...\")\n",
    "model.eval() # Set model to evaluation mode\n",
    "test_predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting on Test Data\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # *** FIX HERE: Squeeze before sigmoid for prediction ***\n",
    "        logits = outputs.logits\n",
    "\n",
    "        preds = torch.sigmoid(logits.squeeze(-1)).cpu().numpy() # Convert logits to probabilities\n",
    "        test_predictions.extend(preds)\n",
    "\n",
    "print(\"Predictions generated.\")\n",
    "\n",
    "# --- Create Submission File ---\n",
    "print(\"Creating submission.csv file...\")\n",
    "submission_df = pd.DataFrame({\n",
    "    'row_id': df_test['row_id'],\n",
    "    'rule_violation': test_predictions\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"submission.csv created successfully!\")\n",
    "print(submission_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 13121456,
     "isSourceIdPinned": false,
     "sourceId": 94635,
     "sourceType": "competition"
    },
    {
     "datasetId": 7984956,
     "sourceId": 12636496,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 497.567905,
   "end_time": "2025-08-01T02:36:01.851850",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-01T02:27:44.283945",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
