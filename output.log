Using NVIDIA CUDA GPU: NVIDIA A100-SXM4-40GB
GPU Memory: 39.56 GB
/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(
tokenizer_config.json: 100%
 25.0/25.0 [00:00<00:00, 2.68kB/s]
config.json: 100%
 615/615 [00:00<00:00, 67.2kB/s]
sentencepiece.bpe.model: 100%
 5.07M/5.07M [00:00<00:00, 5.83MB/s]
tokenizer.json: 100%
 9.10M/9.10M [00:01<00:00, 6.97MB/s]

----- Fold 1 -----
model.safetensors: 100%
 1.12G/1.12G [00:04<00:00, 429MB/s]
Training Epoch 1:   0%|          | 0/406 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Training Epoch 1: 100%|██████████| 406/406 [01:36<00:00,  4.21it/s]
Epoch 1 Loss: 0.6925
Validating Epoch 1: 100%|██████████| 102/102 [00:07<00:00, 14.39it/s]
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
              precision    recall  f1-score   support

         0.0      0.000     0.000     0.000       200
         1.0      0.507     1.000     0.673       206

    accuracy                          0.507       406
   macro avg      0.254     0.500     0.337       406
weighted avg      0.257     0.507     0.342       406

AUC Score: 0.7160
  -> New best Val AUC for Fold 1: 0.7160
Training Epoch 2:   0%|          | 0/406 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Training Epoch 2: 100%|██████████| 406/406 [01:35<00:00,  4.26it/s]
Epoch 2 Loss: 0.6546
Validating Epoch 2: 100%|██████████| 102/102 [00:07<00:00, 14.42it/s]
              precision    recall  f1-score   support

         0.0      0.750     0.705     0.727       200
         1.0      0.729     0.772     0.750       206

    accuracy                          0.739       406
   macro avg      0.740     0.738     0.738       406
weighted avg      0.740     0.739     0.739       406

AUC Score: 0.7957
  -> New best Val AUC for Fold 1: 0.7957
Training Epoch 3:   0%|          | 0/406 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Training Epoch 3: 100%|██████████| 406/406 [01:35<00:00,  4.25it/s]
Epoch 3 Loss: 0.6649
Validating Epoch 3: 100%|██████████| 102/102 [00:07<00:00, 14.40it/s]
              precision    recall  f1-score   support

         0.0      0.846     0.630     0.722       200
         1.0      0.712     0.888     0.790       206

    accuracy                          0.761       406
   macro avg      0.779     0.759     0.756       406
weighted avg      0.778     0.761     0.757       406

AUC Score: 0.8153
  -> New best Val AUC for Fold 1: 0.8153
Training Epoch 4:   0%|          | 0/406 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Training Epoch 4: 100%|██████████| 406/406 [01:35<00:00,  4.24it/s]
Epoch 4 Loss: 0.5043
Validating Epoch 4: 100%|██████████| 102/102 [00:07<00:00, 14.43it/s]
              precision    recall  f1-score   support

         0.0      0.853     0.665     0.747       200
         1.0      0.732     0.888     0.803       206

    accuracy                          0.778       406
   macro avg      0.792     0.777     0.775       406
weighted avg      0.791     0.778     0.775       406

AUC Score: 0.8417
  -> New best Val AUC for Fold 1: 0.8417
Training Epoch 5:   0%|          | 0/406 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Training Epoch 5: 100%|██████████| 406/406 [01:35<00:00,  4.23it/s]
Epoch 5 Loss: 0.4245
Validating Epoch 5: 100%|██████████| 102/102 [00:07<00:00, 14.29it/s]
              precision    recall  f1-score   support

         0.0      0.849     0.705     0.770       200
         1.0      0.754     0.879     0.812       206

    accuracy                          0.793       406
   macro avg      0.802     0.792     0.791       406
weighted avg      0.801     0.793     0.791       406

AUC Score: 0.8359
Training Epoch 6:   0%|          | 0/406 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Training Epoch 6: 100%|██████████| 406/406 [01:35<00:00,  4.24it/s]
Epoch 6 Loss: 0.3747
Validating Epoch 6: 100%|██████████| 102/102 [00:07<00:00, 14.45it/s]
              precision    recall  f1-score   support

         0.0      0.841     0.740     0.787       200
         1.0      0.774     0.864     0.817       206

    accuracy                          0.803       406
   macro avg      0.807     0.802     0.802       406
weighted avg      0.807     0.803     0.802       406

AUC Score: 0.8447
  -> New best Val AUC for Fold 1: 0.8447
Fold 1 Best Val AUC: 0.8447
Fold 1 OOF Prediction:   0%|          | 0/102 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Fold 1 OOF Prediction: 100%|██████████| 102/102 [00:07<00:00, 14.48it/s]
Fold 1 OOF AUC Check: 0.8447 (Must match Best Val AUC)
Fold 1 Test Prediction:   0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Fold 1 Test Prediction: 100%|██████████| 1/1 [00:00<00:00,  6.91it/s]

----- Fold 2 -----
Training Epoch 1: 100%|██████████| 406/406 [01:35<00:00,  4.23it/s]
Epoch 1 Loss: 0.6909
Validating Epoch 1: 100%|██████████| 102/102 [00:07<00:00, 14.19it/s]
              precision    recall  f1-score   support

         0.0      0.551     0.515     0.532       200
         1.0      0.557     0.592     0.574       206

    accuracy                          0.554       406
   macro avg      0.554     0.554     0.553       406
weighted avg      0.554     0.554     0.554       406

AUC Score: 0.5861
  -> New best Val AUC for Fold 2: 0.5861
Training Epoch 2:   0%|          | 0/406 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Training Epoch 2: 100%|██████████| 406/406 [01:35<00:00,  4.25it/s]
Epoch 2 Loss: 0.6714
Validating Epoch 2: 100%|██████████| 102/102 [00:07<00:00, 14.54it/s]
              precision    recall  f1-score   support

         0.0      0.581     0.680     0.627       200
         1.0      0.628     0.524     0.571       206

    accuracy                          0.601       406
   macro avg      0.605     0.602     0.599       406
weighted avg      0.605     0.601     0.599       406

AUC Score: 0.6930
  -> New best Val AUC for Fold 2: 0.6930
Training Epoch 3:   0%|          | 0/406 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Training Epoch 3: 100%|██████████| 406/406 [01:35<00:00,  4.24it/s]
Epoch 3 Loss: 0.5943
Validating Epoch 3: 100%|██████████| 102/102 [00:07<00:00, 14.42it/s]
              precision    recall  f1-score   support

         0.0      0.826     0.615     0.705       200
         1.0      0.700     0.874     0.778       206

    accuracy                          0.746       406
   macro avg      0.763     0.744     0.741       406
weighted avg      0.762     0.746     0.742       406

AUC Score: 0.8136
  -> New best Val AUC for Fold 2: 0.8136
Training Epoch 4:   0%|          | 0/406 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Training Epoch 4: 100%|██████████| 406/406 [01:35<00:00,  4.24it/s]
Epoch 4 Loss: 0.4666
Validating Epoch 4: 100%|██████████| 102/102 [00:07<00:00, 14.34it/s]
              precision    recall  f1-score   support

         0.0      0.846     0.605     0.706       200
         1.0      0.700     0.893     0.785       206

    accuracy                          0.751       406
   macro avg      0.773     0.749     0.745       406
weighted avg      0.772     0.751     0.746       406

AUC Score: 0.8476
  -> New best Val AUC for Fold 2: 0.8476
Training Epoch 5:   0%|          | 0/406 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Training Epoch 5: 100%|██████████| 406/406 [01:35<00:00,  4.25it/s]
Epoch 5 Loss: 0.3686
Validating Epoch 5: 100%|██████████| 102/102 [00:07<00:00, 14.41it/s]
              precision    recall  f1-score   support

         0.0      0.791     0.755     0.772       200
         1.0      0.772     0.806     0.789       206

    accuracy                          0.781       406
   macro avg      0.781     0.780     0.780       406
weighted avg      0.781     0.781     0.781       406

AUC Score: 0.8575
  -> New best Val AUC for Fold 2: 0.8575
Training Epoch 6:   0%|          | 0/406 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Training Epoch 6: 100%|██████████| 406/406 [01:35<00:00,  4.25it/s]
Epoch 6 Loss: 0.3102
Validating Epoch 6: 100%|██████████| 102/102 [00:07<00:00, 14.38it/s]
              precision    recall  f1-score   support

         0.0      0.823     0.720     0.768       200
         1.0      0.758     0.850     0.801       206

    accuracy                          0.786       406
   macro avg      0.790     0.785     0.784       406
weighted avg      0.790     0.786     0.785       406

AUC Score: 0.8527
Fold 2 Best Val AUC: 0.8575
Fold 2 OOF Prediction:   0%|          | 0/102 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Fold 2 OOF Prediction: 100%|██████████| 102/102 [00:07<00:00, 14.41it/s]
Fold 2 OOF AUC Check: 0.8527 (Must match Best Val AUC)
Fold 2 Test Prediction:   0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Fold 2 Test Prediction: 100%|██████████| 1/1 [00:00<00:00,  6.86it/s]

----- Fold 3 -----
Training Epoch 1: 100%|██████████| 406/406 [01:36<00:00,  4.23it/s]
Epoch 1 Loss: 0.6907
Validating Epoch 1: 100%|██████████| 102/102 [00:07<00:00, 14.40it/s]
              precision    recall  f1-score   support

         0.0      0.535     0.605     0.568       200
         1.0      0.561     0.490     0.523       206

    accuracy                          0.547       406
   macro avg      0.548     0.548     0.546       406
weighted avg      0.548     0.547     0.545       406

AUC Score: 0.6062
  -> New best Val AUC for Fold 3: 0.6062
Training Epoch 2:   0%|          | 0/406 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Training Epoch 2: 100%|██████████| 406/406 [01:35<00:00,  4.27it/s]
Epoch 2 Loss: 0.6540
Validating Epoch 2: 100%|██████████| 102/102 [00:07<00:00, 14.43it/s]
              precision    recall  f1-score   support

         0.0      0.749     0.715     0.731       200
         1.0      0.735     0.767     0.751       206

    accuracy                          0.741       406
   macro avg      0.742     0.741     0.741       406
weighted avg      0.742     0.741     0.741       406

AUC Score: 0.7995
  -> New best Val AUC for Fold 3: 0.7995
Training Epoch 3:   0%|          | 0/406 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Training Epoch 3: 100%|██████████| 406/406 [01:35<00:00,  4.25it/s]
Epoch 3 Loss: 0.5337
Validating Epoch 3: 100%|██████████| 102/102 [00:07<00:00, 14.29it/s]
              precision    recall  f1-score   support

         0.0      0.830     0.780     0.804       200
         1.0      0.798     0.845     0.821       206

    accuracy                          0.813       406
   macro avg      0.814     0.812     0.812       406
weighted avg      0.814     0.813     0.813       406

AUC Score: 0.8765
  -> New best Val AUC for Fold 3: 0.8765
Training Epoch 4:   0%|          | 0/406 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Training Epoch 4: 100%|██████████| 406/406 [01:35<00:00,  4.25it/s]
Epoch 4 Loss: 0.4241
Validating Epoch 4: 100%|██████████| 102/102 [00:07<00:00, 14.35it/s]
              precision    recall  f1-score   support

         0.0      0.789     0.840     0.814       200
         1.0      0.834     0.782     0.807       206

    accuracy                          0.810       406
   macro avg      0.811     0.811     0.810       406
weighted avg      0.812     0.810     0.810       406

AUC Score: 0.8600
Training Epoch 5:   0%|          | 0/406 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Training Epoch 5: 100%|██████████| 406/406 [01:35<00:00,  4.26it/s]
Epoch 5 Loss: 0.3406
Validating Epoch 5: 100%|██████████| 102/102 [00:07<00:00, 14.49it/s]
              precision    recall  f1-score   support

         0.0      0.854     0.730     0.787       200
         1.0      0.770     0.879     0.821       206

    accuracy                          0.805       406
   macro avg      0.812     0.804     0.804       406
weighted avg      0.811     0.805     0.804       406

AUC Score: 0.8680
Training Epoch 6:   0%|          | 0/406 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Training Epoch 6: 100%|██████████| 406/406 [01:35<00:00,  4.25it/s]
Epoch 6 Loss: 0.2969
Validating Epoch 6: 100%|██████████| 102/102 [00:07<00:00, 14.40it/s]
              precision    recall  f1-score   support

         0.0      0.832     0.765     0.797       200
         1.0      0.788     0.850     0.818       206

    accuracy                          0.808       406
   macro avg      0.810     0.807     0.807       406
weighted avg      0.810     0.808     0.807       406

AUC Score: 0.8815
  -> New best Val AUC for Fold 3: 0.8815
Fold 3 Best Val AUC: 0.8815
Fold 3 OOF Prediction:   0%|          | 0/102 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Fold 3 OOF Prediction: 100%|██████████| 102/102 [00:07<00:00, 14.30it/s]
Fold 3 OOF AUC Check: 0.8815 (Must match Best Val AUC)
Fold 3 Test Prediction:   0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Fold 3 Test Prediction: 100%|██████████| 1/1 [00:00<00:00,  6.90it/s]

----- Fold 4 -----
Training Epoch 1: 100%|██████████| 406/406 [01:35<00:00,  4.25it/s]
Epoch 1 Loss: 0.6917
Validating Epoch 1: 100%|██████████| 102/102 [00:07<00:00, 14.26it/s]
              precision    recall  f1-score   support

         0.0      0.769     0.452     0.570       199
         1.0      0.623     0.870     0.726       207

    accuracy                          0.665       406
   macro avg      0.696     0.661     0.648       406
weighted avg      0.695     0.665     0.649       406

AUC Score: 0.6888
  -> New best Val AUC for Fold 4: 0.6888
Training Epoch 2:   0%|          | 0/406 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Training Epoch 2: 100%|██████████| 406/406 [01:36<00:00,  4.21it/s]
Epoch 2 Loss: 0.6199
Validating Epoch 2: 100%|██████████| 102/102 [00:07<00:00, 14.22it/s]
              precision    recall  f1-score   support

         0.0      0.870     0.538     0.665       199
         1.0      0.675     0.923     0.780       207

    accuracy                          0.734       406
   macro avg      0.772     0.730     0.722       406
weighted avg      0.770     0.734     0.723       406

AUC Score: 0.8139
  -> New best Val AUC for Fold 4: 0.8139
Training Epoch 3:   0%|          | 0/406 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Training Epoch 3: 100%|██████████| 406/406 [01:35<00:00,  4.24it/s]
Epoch 3 Loss: 0.4960
Validating Epoch 3: 100%|██████████| 102/102 [00:07<00:00, 14.31it/s]
              precision    recall  f1-score   support

         0.0      0.855     0.623     0.721       199
         1.0      0.713     0.899     0.795       207

    accuracy                          0.764       406
   macro avg      0.784     0.761     0.758       406
weighted avg      0.783     0.764     0.759       406

AUC Score: 0.8621
  -> New best Val AUC for Fold 4: 0.8621
Training Epoch 4:   0%|          | 0/406 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Training Epoch 4: 100%|██████████| 406/406 [01:36<00:00,  4.21it/s]
Epoch 4 Loss: 0.3805
Validating Epoch 4: 100%|██████████| 102/102 [00:07<00:00, 13.91it/s]
              precision    recall  f1-score   support

         0.0      0.877     0.643     0.742       199
         1.0      0.727     0.913     0.809       207

    accuracy                          0.781       406
   macro avg      0.802     0.778     0.776       406
weighted avg      0.800     0.781     0.776       406

AUC Score: 0.8641
  -> New best Val AUC for Fold 4: 0.8641
Training Epoch 5:   0%|          | 0/406 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Training Epoch 5: 100%|██████████| 406/406 [01:36<00:00,  4.20it/s]
Epoch 5 Loss: 0.3222
Validating Epoch 5: 100%|██████████| 102/102 [00:07<00:00, 14.19it/s]
              precision    recall  f1-score   support

         0.0      0.815     0.754     0.783       199
         1.0      0.779     0.836     0.807       207

    accuracy                          0.796       406
   macro avg      0.797     0.795     0.795       406
weighted avg      0.797     0.796     0.795       406

AUC Score: 0.8764
  -> New best Val AUC for Fold 4: 0.8764
Training Epoch 6:   0%|          | 0/406 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Training Epoch 6: 100%|██████████| 406/406 [01:36<00:00,  4.23it/s]
Epoch 6 Loss: 0.2793
Validating Epoch 6: 100%|██████████| 102/102 [00:07<00:00, 14.40it/s]
              precision    recall  f1-score   support

         0.0      0.813     0.744     0.777       199
         1.0      0.772     0.836     0.803       207

    accuracy                          0.791       406
   macro avg      0.793     0.790     0.790       406
weighted avg      0.792     0.791     0.790       406

AUC Score: 0.8724
Fold 4 Best Val AUC: 0.8764
Fold 4 OOF Prediction:   0%|          | 0/102 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Fold 4 OOF Prediction: 100%|██████████| 102/102 [00:07<00:00, 14.40it/s]
Fold 4 OOF AUC Check: 0.8724 (Must match Best Val AUC)
Fold 4 Test Prediction:   0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Fold 4 Test Prediction: 100%|██████████| 1/1 [00:00<00:00,  6.87it/s]

----- Fold 5 -----
Training Epoch 1: 100%|██████████| 406/406 [01:35<00:00,  4.25it/s]
Epoch 1 Loss: 0.6847
Validating Epoch 1: 100%|██████████| 102/102 [00:07<00:00, 14.48it/s]
              precision    recall  f1-score   support

         0.0      0.589     0.563     0.576       199
         1.0      0.595     0.621     0.608       206

    accuracy                          0.593       405
   macro avg      0.592     0.592     0.592       405
weighted avg      0.592     0.593     0.592       405

AUC Score: 0.6564
  -> New best Val AUC for Fold 5: 0.6564
Training Epoch 2:   0%|          | 0/406 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Training Epoch 2: 100%|██████████| 406/406 [01:35<00:00,  4.24it/s]
Epoch 2 Loss: 0.5927
Validating Epoch 2: 100%|██████████| 102/102 [00:07<00:00, 14.41it/s]
              precision    recall  f1-score   support

         0.0      0.733     0.744     0.738       199
         1.0      0.749     0.738     0.743       206

    accuracy                          0.741       405
   macro avg      0.741     0.741     0.741       405
weighted avg      0.741     0.741     0.741       405

AUC Score: 0.8252
  -> New best Val AUC for Fold 5: 0.8252
Training Epoch 3:   0%|          | 0/406 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Training Epoch 3: 100%|██████████| 406/406 [01:35<00:00,  4.24it/s]
Epoch 3 Loss: 0.4643
Validating Epoch 3: 100%|██████████| 102/102 [00:07<00:00, 14.56it/s]
              precision    recall  f1-score   support

         0.0      0.804     0.724     0.762       199
         1.0      0.757     0.830     0.792       206

    accuracy                          0.778       405
   macro avg      0.781     0.777     0.777       405
weighted avg      0.780     0.778     0.777       405

AUC Score: 0.8439
  -> New best Val AUC for Fold 5: 0.8439
Training Epoch 4:   0%|          | 0/406 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Training Epoch 4: 100%|██████████| 406/406 [01:35<00:00,  4.24it/s]
Epoch 4 Loss: 0.3876
Validating Epoch 4: 100%|██████████| 102/102 [00:07<00:00, 14.44it/s]
              precision    recall  f1-score   support

         0.0      0.795     0.643     0.711       199
         1.0      0.709     0.840     0.769       206

    accuracy                          0.743       405
   macro avg      0.752     0.742     0.740       405
weighted avg      0.751     0.743     0.740       405

AUC Score: 0.8563
  -> New best Val AUC for Fold 5: 0.8563
Training Epoch 5:   0%|          | 0/406 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Training Epoch 5: 100%|██████████| 406/406 [01:35<00:00,  4.25it/s]
Epoch 5 Loss: 0.3072
Validating Epoch 5: 100%|██████████| 102/102 [00:06<00:00, 14.57it/s]
              precision    recall  f1-score   support

         0.0      0.820     0.709     0.760       199
         1.0      0.751     0.850     0.797       206

    accuracy                          0.780       405
   macro avg      0.785     0.779     0.779       405
weighted avg      0.785     0.780     0.779       405

AUC Score: 0.8548
Training Epoch 6:   0%|          | 0/406 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Training Epoch 6: 100%|██████████| 406/406 [01:35<00:00,  4.26it/s]
Epoch 6 Loss: 0.2670
Validating Epoch 6: 100%|██████████| 102/102 [00:07<00:00, 14.40it/s]
              precision    recall  f1-score   support

         0.0      0.780     0.749     0.764       199
         1.0      0.766     0.796     0.781       206

    accuracy                          0.773       405
   macro avg      0.773     0.772     0.773       405
weighted avg      0.773     0.773     0.773       405

AUC Score: 0.8556
Fold 5 Best Val AUC: 0.8563
Fold 5 OOF Prediction:   0%|          | 0/102 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Fold 5 OOF Prediction: 100%|██████████| 102/102 [00:07<00:00, 14.42it/s]
Fold 5 OOF AUC Check: 0.8556 (Must match Best Val AUC)
Fold 5 Test Prediction:   0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Fold 5 Test Prediction: 100%|██████████| 1/1 [00:00<00:00,  6.87it/s]